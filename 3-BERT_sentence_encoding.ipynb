{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Embedding, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import talos\n",
    "from talos import Predict\n",
    "\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carichiamo i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = path.abspath(\"\")+'/dataset/bert/'\n",
    "\n",
    "train_data = pd.read_csv(PATH+'train.csv')\n",
    "train_vectors = np.load(PATH+'bert_train_vectors.npy')\n",
    "test_vectors = pd.read_csv(PATH+'bert_test_vectors.csv', header=None)\n",
    "test_labels = pd.read_csv(PATH+'bert_test_labels.csv', header=None)\n",
    "\n",
    "list_classes = list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_labels = train_data[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6243, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train data in train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656, 768)\n",
      "(127656, 6)\n",
      "(31915, 768)\n",
      "(31915, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_vectors, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiamo le metriche custom da utilizzare durante la Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiamo le funzioni per visualizzare i plot che mostrano l'andamento dell'apprendimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(x_plot, network_history.history['loss'])\n",
    "    plt.plot(x_plot, network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(x_plot, network_history.history['accuracy'])\n",
    "    plt.plot(x_plot, network_history.history['val_accuracy'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1measure')\n",
    "    plt.plot(x_plot, network_history.history['fmeasure'])\n",
    "    plt.plot(x_plot, network_history.history['val_fmeasure'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, a questo punto possiamo iniziare a OTTIMIZZARE!\n",
    "### Dobbiamo definire la rete come una funzione black box: la funzione prende in input i dati e gli iperparametri da ottimizare e restituisce la history e il modello stesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos\n",
    "from keras import optimizers\n",
    "# define the model function:\n",
    "def toxic_clf(x_train, y_train, x_val, y_val, params):\n",
    "    sequence_input = Input(shape=(x_train.shape[1],), name='input')\n",
    "    layer = Dense(params['first_hidden'], activation=\"relu\", name='hidden_1')(sequence_input)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Dense(params['second_hidden'], activation=\"relu\", name='hidden_2')(layer)\n",
    "    layer = Dense(params['third_hidden'], activation=\"relu\", name='hidden_3')(layer)\n",
    "    output = Dense(6, activation=\"sigmoid\", name='output')(layer)\n",
    "    model = Model(sequence_input, output)\n",
    "    \n",
    "    # Optimizer\n",
    "    opt = optimizers.Adam(learning_rate=0.005, amsgrad=True)\n",
    "\n",
    "    # Monitor on learning rate\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                patience=2, \n",
    "                                                verbose=1, \n",
    "                                                mode='min',\n",
    "                                                factor=0.5, \n",
    "                                                min_lr=0.00001)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy', fmeasure])\n",
    "    \n",
    "    nepochs = params['round_epochs']\n",
    "    batch = params['batch_size']\n",
    "    history = model.fit(x_train, y_train, epochs = nepochs, batch_size = batch, \n",
    "                        validation_data = (x_val, y_val), callbacks=[learning_rate_reduction])\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiamo prima di tutto un insieme di iperparametri indicativo secondo le nostre esperienze con questo dataset\n",
    "### Dopo una esecuzione con un budget limitato sarà possibile analizzare i risultati per ridurre lo spazio di ricerca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the params dict\n",
    "params = {'first_hidden':[200, 400, 560, 768, 800],\n",
    "     'batch_size': [32, 64, 128],\n",
    "     'round_epochs': [12, 15, 18],\n",
    "     'second_hidden': [100, 200, 300],\n",
    "     'third_hidden': [20, 32, 50]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procediamo con un modello random forest con budget 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 7s 53us/step - loss: 0.0618 - accuracy: 0.9771 - fmeasure: 0.6227 - val_loss: 0.0556 - val_accuracy: 0.9780 - val_fmeasure: 0.6083\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 7s 53us/step - loss: 0.0546 - accuracy: 0.9794 - fmeasure: 0.6640 - val_loss: 0.0536 - val_accuracy: 0.9794 - val_fmeasure: 0.6445\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 7s 53us/step - loss: 0.0523 - accuracy: 0.9802 - fmeasure: 0.6791 - val_loss: 0.0519 - val_accuracy: 0.9802 - val_fmeasure: 0.6873\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 6s 51us/step - loss: 0.0512 - accuracy: 0.9804 - fmeasure: 0.6867 - val_loss: 0.0560 - val_accuracy: 0.9789 - val_fmeasure: 0.6301\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0502 - accuracy: 0.9807 - fmeasure: 0.6919 - val_loss: 0.0515 - val_accuracy: 0.9804 - val_fmeasure: 0.7090\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 7s 53us/step - loss: 0.0489 - accuracy: 0.9813 - fmeasure: 0.6999 - val_loss: 0.0501 - val_accuracy: 0.9805 - val_fmeasure: 0.6825\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 7s 53us/step - loss: 0.0481 - accuracy: 0.9817 - fmeasure: 0.7100 - val_loss: 0.0509 - val_accuracy: 0.9805 - val_fmeasure: 0.6903\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 6s 51us/step - loss: 0.0473 - accuracy: 0.9817 - fmeasure: 0.7110 - val_loss: 0.0505 - val_accuracy: 0.9807 - val_fmeasure: 0.7072\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 7s 52us/step - loss: 0.0443 - accuracy: 0.9829 - fmeasure: 0.7312 - val_loss: 0.0500 - val_accuracy: 0.9809 - val_fmeasure: 0.6945\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 7s 52us/step - loss: 0.0432 - accuracy: 0.9831 - fmeasure: 0.7372 - val_loss: 0.0510 - val_accuracy: 0.9807 - val_fmeasure: 0.6962\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 7s 51us/step - loss: 0.0428 - accuracy: 0.9833 - fmeasure: 0.7421 - val_loss: 0.0510 - val_accuracy: 0.9807 - val_fmeasure: 0.6921\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 7s 52us/step - loss: 0.0406 - accuracy: 0.9841 - fmeasure: 0.7554 - val_loss: 0.0505 - val_accuracy: 0.9809 - val_fmeasure: 0.7015\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 7s 53us/step - loss: 0.0400 - accuracy: 0.9843 - fmeasure: 0.7593 - val_loss: 0.0503 - val_accuracy: 0.9809 - val_fmeasure: 0.7079\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0386 - accuracy: 0.9848 - fmeasure: 0.7677 - val_loss: 0.0503 - val_accuracy: 0.9809 - val_fmeasure: 0.7067\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 7s 51us/step - loss: 0.0379 - accuracy: 0.9852 - fmeasure: 0.7741 - val_loss: 0.0507 - val_accuracy: 0.9811 - val_fmeasure: 0.7111\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 7s 53us/step - loss: 0.0372 - accuracy: 0.9853 - fmeasure: 0.7755 - val_loss: 0.0509 - val_accuracy: 0.9811 - val_fmeasure: 0.7095\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 7s 51us/step - loss: 0.0369 - accuracy: 0.9855 - fmeasure: 0.7788 - val_loss: 0.0510 - val_accuracy: 0.9811 - val_fmeasure: 0.7108\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 7s 51us/step - loss: 0.0364 - accuracy: 0.9857 - fmeasure: 0.7811 - val_loss: 0.0510 - val_accuracy: 0.9811 - val_fmeasure: 0.7143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 1/4 [01:59<05:59, 119.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/12\n",
      "127656/127656 [==============================] - 23s 181us/step - loss: 0.0624 - accuracy: 0.9773 - fmeasure: 0.5479 - val_loss: 0.0546 - val_accuracy: 0.9793 - val_fmeasure: 0.6007\n",
      "Epoch 2/12\n",
      "127656/127656 [==============================] - 22s 176us/step - loss: 0.0560 - accuracy: 0.9792 - fmeasure: 0.5843 - val_loss: 0.0556 - val_accuracy: 0.9794 - val_fmeasure: 0.5827\n",
      "Epoch 3/12\n",
      "127656/127656 [==============================] - 23s 181us/step - loss: 0.0539 - accuracy: 0.9798 - fmeasure: 0.5957 - val_loss: 0.0538 - val_accuracy: 0.9794 - val_fmeasure: 0.5908\n",
      "Epoch 4/12\n",
      "127656/127656 [==============================] - 23s 178us/step - loss: 0.0527 - accuracy: 0.9801 - fmeasure: 0.6021 - val_loss: 0.0521 - val_accuracy: 0.9804 - val_fmeasure: 0.6241\n",
      "Epoch 5/12\n",
      "127656/127656 [==============================] - 23s 180us/step - loss: 0.0517 - accuracy: 0.9804 - fmeasure: 0.6164 - val_loss: 0.0522 - val_accuracy: 0.9799 - val_fmeasure: 0.6154\n",
      "Epoch 6/12\n",
      "127656/127656 [==============================] - 23s 180us/step - loss: 0.0509 - accuracy: 0.9808 - fmeasure: 0.6181 - val_loss: 0.0531 - val_accuracy: 0.9799 - val_fmeasure: 0.5910\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/12\n",
      "127656/127656 [==============================] - 23s 177us/step - loss: 0.0478 - accuracy: 0.9816 - fmeasure: 0.6354 - val_loss: 0.0513 - val_accuracy: 0.9806 - val_fmeasure: 0.6214\n",
      "Epoch 8/12\n",
      "127656/127656 [==============================] - 22s 176us/step - loss: 0.0470 - accuracy: 0.9819 - fmeasure: 0.6413 - val_loss: 0.0510 - val_accuracy: 0.9806 - val_fmeasure: 0.6237\n",
      "Epoch 9/12\n",
      "127656/127656 [==============================] - 23s 183us/step - loss: 0.0462 - accuracy: 0.9822 - fmeasure: 0.6503 - val_loss: 0.0504 - val_accuracy: 0.9808 - val_fmeasure: 0.6272\n",
      "Epoch 10/12\n",
      "127656/127656 [==============================] - 24s 188us/step - loss: 0.0456 - accuracy: 0.9824 - fmeasure: 0.6489 - val_loss: 0.0504 - val_accuracy: 0.9807 - val_fmeasure: 0.6385\n",
      "Epoch 11/12\n",
      "127656/127656 [==============================] - 23s 181us/step - loss: 0.0452 - accuracy: 0.9826 - fmeasure: 0.6586 - val_loss: 0.0504 - val_accuracy: 0.9806 - val_fmeasure: 0.6486\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 12/12\n",
      "127656/127656 [==============================] - 24s 185us/step - loss: 0.0428 - accuracy: 0.9833 - fmeasure: 0.6706 - val_loss: 0.0508 - val_accuracy: 0.9806 - val_fmeasure: 0.6476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [06:37<05:34, 167.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 21s 164us/step - loss: 0.0624 - accuracy: 0.9774 - fmeasure: 0.6094 - val_loss: 0.0579 - val_accuracy: 0.9775 - val_fmeasure: 0.5564\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 20s 157us/step - loss: 0.0556 - accuracy: 0.9793 - fmeasure: 0.6384 - val_loss: 0.0549 - val_accuracy: 0.9791 - val_fmeasure: 0.6703\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 20s 154us/step - loss: 0.0537 - accuracy: 0.9800 - fmeasure: 0.6544 - val_loss: 0.0522 - val_accuracy: 0.9802 - val_fmeasure: 0.6662\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 20s 154us/step - loss: 0.0524 - accuracy: 0.9802 - fmeasure: 0.6605 - val_loss: 0.0539 - val_accuracy: 0.9798 - val_fmeasure: 0.6537\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 20s 155us/step - loss: 0.0512 - accuracy: 0.9805 - fmeasure: 0.6721 - val_loss: 0.0518 - val_accuracy: 0.9803 - val_fmeasure: 0.6531\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 20s 156us/step - loss: 0.0503 - accuracy: 0.9809 - fmeasure: 0.6759 - val_loss: 0.0515 - val_accuracy: 0.9804 - val_fmeasure: 0.6693\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 20s 157us/step - loss: 0.0496 - accuracy: 0.9811 - fmeasure: 0.6803 - val_loss: 0.0517 - val_accuracy: 0.9804 - val_fmeasure: 0.6757\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 20s 153us/step - loss: 0.0488 - accuracy: 0.9813 - fmeasure: 0.6871 - val_loss: 0.0545 - val_accuracy: 0.9795 - val_fmeasure: 0.6313\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 20s 159us/step - loss: 0.0459 - accuracy: 0.9822 - fmeasure: 0.7056 - val_loss: 0.0505 - val_accuracy: 0.9808 - val_fmeasure: 0.6830\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 20s 157us/step - loss: 0.0449 - accuracy: 0.9825 - fmeasure: 0.7103 - val_loss: 0.0511 - val_accuracy: 0.9807 - val_fmeasure: 0.6754\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 21s 163us/step - loss: 0.0444 - accuracy: 0.9826 - fmeasure: 0.7117 - val_loss: 0.0505 - val_accuracy: 0.9809 - val_fmeasure: 0.6770\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 21s 163us/step - loss: 0.0424 - accuracy: 0.9835 - fmeasure: 0.7259 - val_loss: 0.0501 - val_accuracy: 0.9808 - val_fmeasure: 0.6947\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 20s 158us/step - loss: 0.0414 - accuracy: 0.9837 - fmeasure: 0.7308 - val_loss: 0.0521 - val_accuracy: 0.9806 - val_fmeasure: 0.6692\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 20s 160us/step - loss: 0.0409 - accuracy: 0.9838 - fmeasure: 0.7321 - val_loss: 0.0514 - val_accuracy: 0.9806 - val_fmeasure: 0.6831\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 20s 159us/step - loss: 0.0396 - accuracy: 0.9844 - fmeasure: 0.7478 - val_loss: 0.0510 - val_accuracy: 0.9809 - val_fmeasure: 0.6870\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 20s 156us/step - loss: 0.0392 - accuracy: 0.9846 - fmeasure: 0.7507 - val_loss: 0.0511 - val_accuracy: 0.9809 - val_fmeasure: 0.6907\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 21s 164us/step - loss: 0.0383 - accuracy: 0.9848 - fmeasure: 0.7540 - val_loss: 0.0515 - val_accuracy: 0.9809 - val_fmeasure: 0.6895\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 21s 163us/step - loss: 0.0381 - accuracy: 0.9851 - fmeasure: 0.7573 - val_loss: 0.0514 - val_accuracy: 0.9810 - val_fmeasure: 0.6887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [12:41<03:46, 226.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/15\n",
      "127656/127656 [==============================] - 9s 73us/step - loss: 0.0623 - accuracy: 0.9772 - fmeasure: 0.6188 - val_loss: 0.0554 - val_accuracy: 0.9788 - val_fmeasure: 0.6360\n",
      "Epoch 2/15\n",
      "127656/127656 [==============================] - 9s 69us/step - loss: 0.0551 - accuracy: 0.9793 - fmeasure: 0.6621 - val_loss: 0.0536 - val_accuracy: 0.9792 - val_fmeasure: 0.6426\n",
      "Epoch 3/15\n",
      "127656/127656 [==============================] - 9s 70us/step - loss: 0.0527 - accuracy: 0.9799 - fmeasure: 0.6748 - val_loss: 0.0527 - val_accuracy: 0.9793 - val_fmeasure: 0.6870\n",
      "Epoch 4/15\n",
      "127656/127656 [==============================] - 9s 68us/step - loss: 0.0514 - accuracy: 0.9805 - fmeasure: 0.6852 - val_loss: 0.0512 - val_accuracy: 0.9805 - val_fmeasure: 0.6905\n",
      "Epoch 5/15\n",
      "127656/127656 [==============================] - 9s 70us/step - loss: 0.0502 - accuracy: 0.9808 - fmeasure: 0.6931 - val_loss: 0.0521 - val_accuracy: 0.9805 - val_fmeasure: 0.6785\n",
      "Epoch 6/15\n",
      "127656/127656 [==============================] - 9s 69us/step - loss: 0.0496 - accuracy: 0.9810 - fmeasure: 0.6970 - val_loss: 0.0504 - val_accuracy: 0.9807 - val_fmeasure: 0.6933\n",
      "Epoch 7/15\n",
      "127656/127656 [==============================] - 9s 69us/step - loss: 0.0483 - accuracy: 0.9814 - fmeasure: 0.7049 - val_loss: 0.0511 - val_accuracy: 0.9803 - val_fmeasure: 0.6811\n",
      "Epoch 8/15\n",
      "127656/127656 [==============================] - 9s 71us/step - loss: 0.0476 - accuracy: 0.9815 - fmeasure: 0.7062 - val_loss: 0.0514 - val_accuracy: 0.9804 - val_fmeasure: 0.6919\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/15\n",
      "127656/127656 [==============================] - 9s 70us/step - loss: 0.0445 - accuracy: 0.9827 - fmeasure: 0.7287 - val_loss: 0.0500 - val_accuracy: 0.9808 - val_fmeasure: 0.7052\n",
      "Epoch 10/15\n",
      "127656/127656 [==============================] - 9s 69us/step - loss: 0.0433 - accuracy: 0.9831 - fmeasure: 0.7372 - val_loss: 0.0502 - val_accuracy: 0.9808 - val_fmeasure: 0.7103\n",
      "Epoch 11/15\n",
      "127656/127656 [==============================] - 9s 72us/step - loss: 0.0428 - accuracy: 0.9832 - fmeasure: 0.7400 - val_loss: 0.0507 - val_accuracy: 0.9806 - val_fmeasure: 0.6960\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 12/15\n",
      "127656/127656 [==============================] - 9s 69us/step - loss: 0.0404 - accuracy: 0.9842 - fmeasure: 0.7574 - val_loss: 0.0503 - val_accuracy: 0.9808 - val_fmeasure: 0.7055\n",
      "Epoch 13/15\n",
      "127656/127656 [==============================] - 9s 68us/step - loss: 0.0394 - accuracy: 0.9844 - fmeasure: 0.7620 - val_loss: 0.0505 - val_accuracy: 0.9808 - val_fmeasure: 0.7014\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/15\n",
      "127656/127656 [==============================] - 9s 69us/step - loss: 0.0382 - accuracy: 0.9850 - fmeasure: 0.7696 - val_loss: 0.0513 - val_accuracy: 0.9808 - val_fmeasure: 0.7004\n",
      "Epoch 15/15\n",
      "127656/127656 [==============================] - 9s 68us/step - loss: 0.0377 - accuracy: 0.9851 - fmeasure: 0.7714 - val_loss: 0.0512 - val_accuracy: 0.9809 - val_fmeasure: 0.7032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [14:56<00:00, 224.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the scan\n",
    "scan_object = talos.Scan(x=x_train,\n",
    "                         y=y_train,\n",
    "                         x_val=x_val,\n",
    "                         y_val=y_val,\n",
    "                         params=params,\n",
    "                         reduction_method='forrest',\n",
    "                         reduction_metric='val_fmeasure',\n",
    "                         performance_target=['val_fmeasure', 0.8, False],\n",
    "                         model=toxic_clf,\n",
    "                         experiment_name='toxic_comments',\n",
    "                         fraction_limit=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dopo la scan osserviamo i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check values:\n",
      "\n",
      "   round_epochs  val_loss  val_accuracy  val_fmeasure      loss  accuracy  \\\n",
      "0            18  0.051027      0.981127      0.714253  0.036411  0.985661   \n",
      "1            12  0.050806      0.980626      0.647561  0.042782  0.983317   \n",
      "2            18  0.051399      0.980971      0.688748  0.038085  0.985073   \n",
      "3            15  0.051183      0.980861      0.703184  0.037709  0.985068   \n",
      "\n",
      "   fmeasure        lr  batch_size  first_hidden  round_epochs  second_hidden  \\\n",
      "0  0.781102  0.000156         128           400            18            100   \n",
      "1  0.670621  0.001250          32           560            12            100   \n",
      "2  0.757332  0.000312          64           768            18            300   \n",
      "3  0.771434  0.000625         128           560            15            200   \n",
      "\n",
      "   third_hidden  \n",
      "0            32  \n",
      "1            50  \n",
      "2            32  \n",
      "3            50  \n",
      "\n",
      "Entropy:\n",
      "\n",
      "       loss  accuracy  fmeasure        lr\n",
      "0  2.878770  2.890369  2.888487  2.602597\n",
      "1  2.479414  2.484905  2.483400  2.406704\n",
      "2  2.880456  2.890369  2.888569  2.640469\n",
      "3  2.698008  2.708048  2.706265  2.547056\n",
      "\n",
      "Summary:\n",
      "\n",
      "experiment_name          toxic_comments\n",
      "random_method          uniform_mersenne\n",
      "reduction_method                forrest\n",
      "reduction_interval                   50\n",
      "reduction_window                     20\n",
      "reduction_threshold                 0.2\n",
      "reduction_metric           val_fmeasure\n",
      "complete_time            01/07/20/22:52\n",
      "x_shape                   (127656, 768)\n",
      "y_shape                     (127656, 6)\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# accessing the results data frame\n",
    "print('Check values:\\n')\n",
    "print(scan_object.data.head())\n",
    "\n",
    "# accessing epoch entropy values for each round\n",
    "print('\\nEntropy:\\n')\n",
    "print(scan_object.learning_entropy)\n",
    "\n",
    "# access the summary details\n",
    "print('\\nSummary:\\n')\n",
    "print(scan_object.details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_fmeasure</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fmeasure</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>first_hidden</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>second_hidden</th>\n",
       "      <th>third_hidden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.051027</td>\n",
       "      <td>0.981127</td>\n",
       "      <td>0.714253</td>\n",
       "      <td>0.036411</td>\n",
       "      <td>0.985661</td>\n",
       "      <td>0.781102</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>128</td>\n",
       "      <td>400</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.050806</td>\n",
       "      <td>0.980626</td>\n",
       "      <td>0.647561</td>\n",
       "      <td>0.042782</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>0.670621</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>32</td>\n",
       "      <td>560</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.051399</td>\n",
       "      <td>0.980971</td>\n",
       "      <td>0.688748</td>\n",
       "      <td>0.038085</td>\n",
       "      <td>0.985073</td>\n",
       "      <td>0.757332</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>64</td>\n",
       "      <td>768</td>\n",
       "      <td>18</td>\n",
       "      <td>300</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.051183</td>\n",
       "      <td>0.980861</td>\n",
       "      <td>0.703184</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.985068</td>\n",
       "      <td>0.771434</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>128</td>\n",
       "      <td>560</td>\n",
       "      <td>15</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs  val_loss  val_accuracy  val_fmeasure      loss  accuracy  \\\n",
       "0            18  0.051027      0.981127      0.714253  0.036411  0.985661   \n",
       "1            12  0.050806      0.980626      0.647561  0.042782  0.983317   \n",
       "2            18  0.051399      0.980971      0.688748  0.038085  0.985073   \n",
       "3            15  0.051183      0.980861      0.703184  0.037709  0.985068   \n",
       "\n",
       "   fmeasure        lr  batch_size  first_hidden  round_epochs  second_hidden  \\\n",
       "0  0.781102  0.000156         128           400            18            100   \n",
       "1  0.670621  0.001250          32           560            12            100   \n",
       "2  0.757332  0.000312          64           768            18            300   \n",
       "3  0.771434  0.000625         128           560            15            200   \n",
       "\n",
       "   third_hidden  \n",
       "0            32  \n",
       "1            50  \n",
       "2            32  \n",
       "3            50  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze scan object\n",
    "analyze_object = talos.Analyze(scan_object)\n",
    "# Results\n",
    "analyze_object.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Osserviamo i risultati ottenuti in termine di max fmeasure, validation fmeasure e accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest val accuracy:  0.9811274409294128\n",
      "highest val fmeasure:  0.7142529487609863\n",
      "highest fmeasure:  0.7811022996902466\n",
      "Best params:  [[1.80000000e+01 1.80000000e+01 3.20000000e+01 1.56249997e-04\n",
      "  4.00000000e+02 1.28000000e+02 7.81102300e-01 1.00000000e+02\n",
      "  0.00000000e+00]\n",
      " [1.50000000e+01 1.50000000e+01 5.00000000e+01 6.24999986e-04\n",
      "  5.60000000e+02 1.28000000e+02 7.71433651e-01 2.00000000e+02\n",
      "  1.00000000e+00]\n",
      " [1.80000000e+01 1.80000000e+01 3.20000000e+01 3.12499993e-04\n",
      "  7.68000000e+02 6.40000000e+01 7.57332206e-01 3.00000000e+02\n",
      "  2.00000000e+00]\n",
      " [1.20000000e+01 1.20000000e+01 5.00000000e+01 1.24999997e-03\n",
      "  5.60000000e+02 3.20000000e+01 6.70621276e-01 1.00000000e+02\n",
      "  3.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# get the highest result for important metrics\n",
    "print(\"highest val accuracy: \",analyze_object.high('val_accuracy'))\n",
    "print(\"highest val fmeasure: \",analyze_object.high('val_fmeasure'))\n",
    "print(\"highest fmeasure: \", analyze_object.high('fmeasure'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ovviamente l'accuracy è alta, 98% rispetto ad una numerosità del 89% della classe di maggioranza vuol dire che il nostro modello sta facendo bene. \n",
    "### Osserviamo ora alcune analisi per capire come comportarci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 684x684 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# heatmap correlation\n",
    "analyze_object.plot_corr('val_fmeasure', ['loss', 'val_loss', 'accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possiamo osservare come val_fmeasure sia direttamente proporzionale al numero di epoche e alla dimensione di batch. Inoltre è inversamente proporzionale alla dimensione del primo hidden layer, e al learning rate. Infine è circa neutro a quella del secondo e del terzo hidden.\n",
    "### Cosa vuol dire? Beh, che è possibile ridurre lo spazio di ricerca andando a rimuovere dal dict le epoche minori, le batch minori, le dimensioni dei layer maggiori, e magari iniziare il LR Reducer con un learning rate più basso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlliamo però più nel dettaglio per farci un'idea anche dei range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x475.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze the Kernel Density Estimator with validation fmeasure and the first hidden layer dimension\n",
    "analyze_object.plot_kde('first_hidden', 'val_fmeasure')\n",
    "\n",
    "# The first is x-axis, the second is y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfetto, come avevamo immaginato, possiamo osservare che i valori più alti di val f1 sono stati raggiunti con 400 neuroni nel primo hidden layer. \n",
    "### Controlliamo anche il batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x475.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze the Kernel Density Estimator with validation fmeasure and the batch_size\n",
    "analyze_object.plot_kde('batch_size', 'val_fmeasure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benissimo, dovremmo preferire dimensioni superiori al 100: proveremo solo 128 e 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non ci è possibile controllare lo stesso grafico con il numero di epoche per via di un conflitto con una variabile pre-impostata chiamata round_epochs che abbiamo provato a sovrascrivere (nel plot della correlazione infatti compare due volte). Tuttavia, abbiamo visto come dobbiamo preferire numeri alti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salviamoci il modello migliore in termini di validation fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy package toxic_comments_deploy have been saved.\n"
     ]
    }
   ],
   "source": [
    "talos.Deploy(scan_object=scan_object, model_name='toxic_comments_deploy', metric='val_fmeasure');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = talos.Restore('toxic_comments_deploy.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ora possiamo testare grazie alla funzione model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.round(model.model.predict(test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.99      0.74      0.84      6090\n",
      " severe_toxic       0.42      0.29      0.34       367\n",
      "      obscene       0.84      0.68      0.75      3691\n",
      "       threat       0.75      0.35      0.48       211\n",
      "       insult       0.80      0.61      0.69      3427\n",
      "identity_hate       0.74      0.36      0.49       712\n",
      "\n",
      "  avg / total       0.87      0.66      0.75     14498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = classification_report(test_labels, y_pred, target_names=list_classes)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Già in dopo 4 scan random possiamo osservare un incremento notevole della micro average f1-score rispetto ai modelli precedentemente testati (base e data augmentation). Stampiamo per esteso il valore non arrotondato per avere una idea più precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro avg:  (0.8769500600018462, 0.6552627948682577, 0.7500690853104891, None)\n"
     ]
    }
   ],
   "source": [
    "score = precision_recall_fscore_support(test_labels, y_pred, average='micro')\n",
    "print('Micro avg: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adesso che abbiamo una idea più solida sui nostri iperparametri, possiamo andare a ridurre lo spazio di ricerca ridefinendo il dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiamo un ultima volta gli iperparametri e lanciamo la scan su più combinazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AGAIN the params dict\n",
    "params = {'first_hidden':[300, 400],\n",
    "     'batch_size': [128, 256],\n",
    "     'round_epochs': [18, 20],\n",
    "     'second_hidden': [100, 200],\n",
    "     'third_hidden': [32, 50]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per questa analisi finale utilizziamo un budget maggiore, ovvero 16 iterazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0621 - accuracy: 0.9773 - fmeasure: 0.6173 - val_loss: 0.0554 - val_accuracy: 0.9792 - val_fmeasure: 0.6599\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0555 - accuracy: 0.9794 - fmeasure: 0.6596 - val_loss: 0.0527 - val_accuracy: 0.9800 - val_fmeasure: 0.6762\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0534 - accuracy: 0.9798 - fmeasure: 0.6710 - val_loss: 0.0529 - val_accuracy: 0.9802 - val_fmeasure: 0.6784\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0519 - accuracy: 0.9802 - fmeasure: 0.6794 - val_loss: 0.0523 - val_accuracy: 0.9799 - val_fmeasure: 0.6710\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0507 - accuracy: 0.9806 - fmeasure: 0.6883 - val_loss: 0.0515 - val_accuracy: 0.9803 - val_fmeasure: 0.6891\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0499 - accuracy: 0.9810 - fmeasure: 0.6957 - val_loss: 0.0510 - val_accuracy: 0.9802 - val_fmeasure: 0.6745\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0491 - accuracy: 0.9811 - fmeasure: 0.6981 - val_loss: 0.0506 - val_accuracy: 0.9804 - val_fmeasure: 0.7007\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0486 - accuracy: 0.9812 - fmeasure: 0.6994 - val_loss: 0.0509 - val_accuracy: 0.9803 - val_fmeasure: 0.6833\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 6s 47us/step - loss: 0.0478 - accuracy: 0.9816 - fmeasure: 0.7072 - val_loss: 0.0538 - val_accuracy: 0.9796 - val_fmeasure: 0.6457\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 6s 44us/step - loss: 0.0452 - accuracy: 0.9825 - fmeasure: 0.7265 - val_loss: 0.0512 - val_accuracy: 0.9806 - val_fmeasure: 0.6897\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0442 - accuracy: 0.9828 - fmeasure: 0.7301 - val_loss: 0.0506 - val_accuracy: 0.9808 - val_fmeasure: 0.6968\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0425 - accuracy: 0.9833 - fmeasure: 0.7407 - val_loss: 0.0500 - val_accuracy: 0.9808 - val_fmeasure: 0.7030\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0419 - accuracy: 0.9836 - fmeasure: 0.7443 - val_loss: 0.0500 - val_accuracy: 0.9806 - val_fmeasure: 0.7055\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0416 - accuracy: 0.9838 - fmeasure: 0.7472 - val_loss: 0.0502 - val_accuracy: 0.9808 - val_fmeasure: 0.6973\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 6s 47us/step - loss: 0.0404 - accuracy: 0.9841 - fmeasure: 0.7558 - val_loss: 0.0506 - val_accuracy: 0.9808 - val_fmeasure: 0.6995\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0400 - accuracy: 0.9843 - fmeasure: 0.7589 - val_loss: 0.0502 - val_accuracy: 0.9809 - val_fmeasure: 0.7045\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 6s 47us/step - loss: 0.0395 - accuracy: 0.9846 - fmeasure: 0.7631 - val_loss: 0.0503 - val_accuracy: 0.9808 - val_fmeasure: 0.7064\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0391 - accuracy: 0.9846 - fmeasure: 0.7644 - val_loss: 0.0507 - val_accuracy: 0.9808 - val_fmeasure: 0.6995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▋         | 1/16 [01:45<26:25, 105.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 7s 59us/step - loss: 0.0617 - accuracy: 0.9772 - fmeasure: 0.6192 - val_loss: 0.0549 - val_accuracy: 0.9789 - val_fmeasure: 0.6367\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 7s 58us/step - loss: 0.0550 - accuracy: 0.9794 - fmeasure: 0.6629 - val_loss: 0.0528 - val_accuracy: 0.9798 - val_fmeasure: 0.6671\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0530 - accuracy: 0.9801 - fmeasure: 0.6713 - val_loss: 0.0523 - val_accuracy: 0.9800 - val_fmeasure: 0.6646\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0515 - accuracy: 0.9803 - fmeasure: 0.6798 - val_loss: 0.0507 - val_accuracy: 0.9807 - val_fmeasure: 0.6885\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0507 - accuracy: 0.9808 - fmeasure: 0.6876 - val_loss: 0.0520 - val_accuracy: 0.9803 - val_fmeasure: 0.7052\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0497 - accuracy: 0.9810 - fmeasure: 0.6956 - val_loss: 0.0509 - val_accuracy: 0.9804 - val_fmeasure: 0.6941\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0468 - accuracy: 0.9820 - fmeasure: 0.7135 - val_loss: 0.0495 - val_accuracy: 0.9808 - val_fmeasure: 0.7070\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0457 - accuracy: 0.9823 - fmeasure: 0.7183 - val_loss: 0.0498 - val_accuracy: 0.9809 - val_fmeasure: 0.6996\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0450 - accuracy: 0.9824 - fmeasure: 0.7209 - val_loss: 0.0501 - val_accuracy: 0.9808 - val_fmeasure: 0.6899\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0432 - accuracy: 0.9832 - fmeasure: 0.7357 - val_loss: 0.0494 - val_accuracy: 0.9810 - val_fmeasure: 0.7115\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0426 - accuracy: 0.9833 - fmeasure: 0.7371 - val_loss: 0.0503 - val_accuracy: 0.9808 - val_fmeasure: 0.6884\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0419 - accuracy: 0.9837 - fmeasure: 0.7453 - val_loss: 0.0498 - val_accuracy: 0.9809 - val_fmeasure: 0.6999\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0405 - accuracy: 0.9841 - fmeasure: 0.7537 - val_loss: 0.0512 - val_accuracy: 0.9808 - val_fmeasure: 0.6908\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0400 - accuracy: 0.9842 - fmeasure: 0.7546 - val_loss: 0.0502 - val_accuracy: 0.9809 - val_fmeasure: 0.6978\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0391 - accuracy: 0.9846 - fmeasure: 0.7605 - val_loss: 0.0502 - val_accuracy: 0.9810 - val_fmeasure: 0.7120\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0389 - accuracy: 0.9846 - fmeasure: 0.7637 - val_loss: 0.0502 - val_accuracy: 0.9811 - val_fmeasure: 0.7074\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0384 - accuracy: 0.9849 - fmeasure: 0.7686 - val_loss: 0.0503 - val_accuracy: 0.9811 - val_fmeasure: 0.7104\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0384 - accuracy: 0.9849 - fmeasure: 0.7673 - val_loss: 0.0507 - val_accuracy: 0.9810 - val_fmeasure: 0.7064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 2/16 [03:54<26:17, 112.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0629 - accuracy: 0.9768 - fmeasure: 0.6283 - val_loss: 0.0545 - val_accuracy: 0.9790 - val_fmeasure: 0.6510\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0549 - accuracy: 0.9794 - fmeasure: 0.6686 - val_loss: 0.0536 - val_accuracy: 0.9793 - val_fmeasure: 0.6503\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0525 - accuracy: 0.9800 - fmeasure: 0.6823 - val_loss: 0.0530 - val_accuracy: 0.9798 - val_fmeasure: 0.6997\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0516 - accuracy: 0.9805 - fmeasure: 0.6917 - val_loss: 0.0513 - val_accuracy: 0.9803 - val_fmeasure: 0.6991\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0503 - accuracy: 0.9809 - fmeasure: 0.7004 - val_loss: 0.0512 - val_accuracy: 0.9806 - val_fmeasure: 0.7004\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0492 - accuracy: 0.9812 - fmeasure: 0.7059 - val_loss: 0.0509 - val_accuracy: 0.9804 - val_fmeasure: 0.6853\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0486 - accuracy: 0.9815 - fmeasure: 0.7115 - val_loss: 0.0506 - val_accuracy: 0.9802 - val_fmeasure: 0.6897\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0477 - accuracy: 0.9817 - fmeasure: 0.7156 - val_loss: 0.0507 - val_accuracy: 0.9807 - val_fmeasure: 0.7090\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0472 - accuracy: 0.9818 - fmeasure: 0.7171 - val_loss: 0.0507 - val_accuracy: 0.9806 - val_fmeasure: 0.7124\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0443 - accuracy: 0.9828 - fmeasure: 0.7363 - val_loss: 0.0495 - val_accuracy: 0.9810 - val_fmeasure: 0.7138\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0432 - accuracy: 0.9830 - fmeasure: 0.7414 - val_loss: 0.0508 - val_accuracy: 0.9807 - val_fmeasure: 0.6892\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0425 - accuracy: 0.9833 - fmeasure: 0.7448 - val_loss: 0.0503 - val_accuracy: 0.9810 - val_fmeasure: 0.7071\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0406 - accuracy: 0.9839 - fmeasure: 0.7570 - val_loss: 0.0500 - val_accuracy: 0.9809 - val_fmeasure: 0.7059\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0397 - accuracy: 0.9843 - fmeasure: 0.7624 - val_loss: 0.0501 - val_accuracy: 0.9810 - val_fmeasure: 0.7148\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0386 - accuracy: 0.9846 - fmeasure: 0.7680 - val_loss: 0.0510 - val_accuracy: 0.9808 - val_fmeasure: 0.7006\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0381 - accuracy: 0.9849 - fmeasure: 0.7732 - val_loss: 0.0505 - val_accuracy: 0.9809 - val_fmeasure: 0.7080\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0375 - accuracy: 0.9851 - fmeasure: 0.7775 - val_loss: 0.0507 - val_accuracy: 0.9809 - val_fmeasure: 0.7085\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0374 - accuracy: 0.9852 - fmeasure: 0.7771 - val_loss: 0.0508 - val_accuracy: 0.9808 - val_fmeasure: 0.7109\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0370 - accuracy: 0.9855 - fmeasure: 0.7834 - val_loss: 0.0508 - val_accuracy: 0.9809 - val_fmeasure: 0.7109\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0367 - accuracy: 0.9854 - fmeasure: 0.7809 - val_loss: 0.0508 - val_accuracy: 0.9809 - val_fmeasure: 0.7124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|█▉        | 3/16 [05:13<22:12, 102.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0634 - accuracy: 0.9765 - fmeasure: 0.6054 - val_loss: 0.0569 - val_accuracy: 0.9789 - val_fmeasure: 0.6481\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0557 - accuracy: 0.9793 - fmeasure: 0.6587 - val_loss: 0.0532 - val_accuracy: 0.9796 - val_fmeasure: 0.6702\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0533 - accuracy: 0.9799 - fmeasure: 0.6716 - val_loss: 0.0520 - val_accuracy: 0.9801 - val_fmeasure: 0.6857\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0519 - accuracy: 0.9802 - fmeasure: 0.6795 - val_loss: 0.0522 - val_accuracy: 0.9802 - val_fmeasure: 0.6803\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0511 - accuracy: 0.9805 - fmeasure: 0.6853 - val_loss: 0.0514 - val_accuracy: 0.9805 - val_fmeasure: 0.6913\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0501 - accuracy: 0.9810 - fmeasure: 0.6942 - val_loss: 0.0515 - val_accuracy: 0.9802 - val_fmeasure: 0.6998\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0494 - accuracy: 0.9812 - fmeasure: 0.7007 - val_loss: 0.0525 - val_accuracy: 0.9799 - val_fmeasure: 0.6671\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0468 - accuracy: 0.9820 - fmeasure: 0.7146 - val_loss: 0.0505 - val_accuracy: 0.9808 - val_fmeasure: 0.6898\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0456 - accuracy: 0.9824 - fmeasure: 0.7222 - val_loss: 0.0505 - val_accuracy: 0.9807 - val_fmeasure: 0.6867\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0452 - accuracy: 0.9826 - fmeasure: 0.7261 - val_loss: 0.0504 - val_accuracy: 0.9806 - val_fmeasure: 0.6880\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0432 - accuracy: 0.9832 - fmeasure: 0.7348 - val_loss: 0.0503 - val_accuracy: 0.9807 - val_fmeasure: 0.6972\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 6s 47us/step - loss: 0.0425 - accuracy: 0.9835 - fmeasure: 0.7420 - val_loss: 0.0504 - val_accuracy: 0.9807 - val_fmeasure: 0.6938\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0422 - accuracy: 0.9836 - fmeasure: 0.7445 - val_loss: 0.0504 - val_accuracy: 0.9808 - val_fmeasure: 0.6993\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0408 - accuracy: 0.9841 - fmeasure: 0.7531 - val_loss: 0.0507 - val_accuracy: 0.9807 - val_fmeasure: 0.6993\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0403 - accuracy: 0.9844 - fmeasure: 0.7578 - val_loss: 0.0511 - val_accuracy: 0.9807 - val_fmeasure: 0.6985\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0397 - accuracy: 0.9845 - fmeasure: 0.7606 - val_loss: 0.0512 - val_accuracy: 0.9806 - val_fmeasure: 0.6964\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0393 - accuracy: 0.9847 - fmeasure: 0.7650 - val_loss: 0.0514 - val_accuracy: 0.9807 - val_fmeasure: 0.6954\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0389 - accuracy: 0.9848 - fmeasure: 0.7648 - val_loss: 0.0512 - val_accuracy: 0.9808 - val_fmeasure: 0.6976\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0390 - accuracy: 0.9848 - fmeasure: 0.7661 - val_loss: 0.0513 - val_accuracy: 0.9808 - val_fmeasure: 0.6991\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0386 - accuracy: 0.9848 - fmeasure: 0.7655 - val_loss: 0.0513 - val_accuracy: 0.9808 - val_fmeasure: 0.6995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 4/16 [07:10<21:21, 106.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 6s 44us/step - loss: 0.0620 - accuracy: 0.9773 - fmeasure: 0.6189 - val_loss: 0.0612 - val_accuracy: 0.9771 - val_fmeasure: 0.5696\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0554 - accuracy: 0.9793 - fmeasure: 0.6582 - val_loss: 0.0530 - val_accuracy: 0.9796 - val_fmeasure: 0.6546\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0528 - accuracy: 0.9800 - fmeasure: 0.6749 - val_loss: 0.0543 - val_accuracy: 0.9795 - val_fmeasure: 0.6596\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0515 - accuracy: 0.9805 - fmeasure: 0.6863 - val_loss: 0.0520 - val_accuracy: 0.9801 - val_fmeasure: 0.6717\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0504 - accuracy: 0.9807 - fmeasure: 0.6899 - val_loss: 0.0515 - val_accuracy: 0.9801 - val_fmeasure: 0.6657\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0494 - accuracy: 0.9811 - fmeasure: 0.6970 - val_loss: 0.0506 - val_accuracy: 0.9806 - val_fmeasure: 0.6855\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0486 - accuracy: 0.9814 - fmeasure: 0.7031 - val_loss: 0.0506 - val_accuracy: 0.9805 - val_fmeasure: 0.6901\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0480 - accuracy: 0.9816 - fmeasure: 0.7069 - val_loss: 0.0508 - val_accuracy: 0.9807 - val_fmeasure: 0.6943\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0452 - accuracy: 0.9825 - fmeasure: 0.7230 - val_loss: 0.0499 - val_accuracy: 0.9808 - val_fmeasure: 0.6994\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0441 - accuracy: 0.9828 - fmeasure: 0.7297 - val_loss: 0.0509 - val_accuracy: 0.9805 - val_fmeasure: 0.6762\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0435 - accuracy: 0.9830 - fmeasure: 0.7312 - val_loss: 0.0500 - val_accuracy: 0.9808 - val_fmeasure: 0.7063\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0417 - accuracy: 0.9836 - fmeasure: 0.7457 - val_loss: 0.0498 - val_accuracy: 0.9810 - val_fmeasure: 0.7065\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0410 - accuracy: 0.9840 - fmeasure: 0.7515 - val_loss: 0.0501 - val_accuracy: 0.9809 - val_fmeasure: 0.6987\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0399 - accuracy: 0.9844 - fmeasure: 0.7580 - val_loss: 0.0502 - val_accuracy: 0.9810 - val_fmeasure: 0.7040\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0394 - accuracy: 0.9846 - fmeasure: 0.7617 - val_loss: 0.0504 - val_accuracy: 0.9809 - val_fmeasure: 0.7017\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0386 - accuracy: 0.9849 - fmeasure: 0.7678 - val_loss: 0.0505 - val_accuracy: 0.9810 - val_fmeasure: 0.7016\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0384 - accuracy: 0.9848 - fmeasure: 0.7658 - val_loss: 0.0504 - val_accuracy: 0.9810 - val_fmeasure: 0.7062\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0379 - accuracy: 0.9852 - fmeasure: 0.7725 - val_loss: 0.0508 - val_accuracy: 0.9810 - val_fmeasure: 0.7023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|███▏      | 5/16 [08:48<19:06, 104.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 6s 51us/step - loss: 0.0618 - accuracy: 0.9771 - fmeasure: 0.6206 - val_loss: 0.0555 - val_accuracy: 0.9792 - val_fmeasure: 0.6515\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0548 - accuracy: 0.9795 - fmeasure: 0.6607 - val_loss: 0.0529 - val_accuracy: 0.9795 - val_fmeasure: 0.6723\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0528 - accuracy: 0.9801 - fmeasure: 0.6770 - val_loss: 0.0516 - val_accuracy: 0.9802 - val_fmeasure: 0.6784\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0516 - accuracy: 0.9805 - fmeasure: 0.6855 - val_loss: 0.0518 - val_accuracy: 0.9802 - val_fmeasure: 0.6724\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0503 - accuracy: 0.9808 - fmeasure: 0.6909 - val_loss: 0.0511 - val_accuracy: 0.9807 - val_fmeasure: 0.7031\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0493 - accuracy: 0.9810 - fmeasure: 0.6982 - val_loss: 0.0503 - val_accuracy: 0.9807 - val_fmeasure: 0.6922\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0481 - accuracy: 0.9814 - fmeasure: 0.7038 - val_loss: 0.0507 - val_accuracy: 0.9808 - val_fmeasure: 0.7081\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0474 - accuracy: 0.9817 - fmeasure: 0.7069 - val_loss: 0.0513 - val_accuracy: 0.9802 - val_fmeasure: 0.6741\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0445 - accuracy: 0.9827 - fmeasure: 0.7284 - val_loss: 0.0505 - val_accuracy: 0.9806 - val_fmeasure: 0.6895\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0436 - accuracy: 0.9831 - fmeasure: 0.7355 - val_loss: 0.0503 - val_accuracy: 0.9804 - val_fmeasure: 0.7115\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0414 - accuracy: 0.9839 - fmeasure: 0.7490 - val_loss: 0.0504 - val_accuracy: 0.9809 - val_fmeasure: 0.6980\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0406 - accuracy: 0.9841 - fmeasure: 0.7533 - val_loss: 0.0502 - val_accuracy: 0.9808 - val_fmeasure: 0.6970\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0396 - accuracy: 0.9845 - fmeasure: 0.7600 - val_loss: 0.0505 - val_accuracy: 0.9810 - val_fmeasure: 0.7098\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0389 - accuracy: 0.9848 - fmeasure: 0.7686 - val_loss: 0.0509 - val_accuracy: 0.9809 - val_fmeasure: 0.6991\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0379 - accuracy: 0.9852 - fmeasure: 0.7697 - val_loss: 0.0507 - val_accuracy: 0.9810 - val_fmeasure: 0.7100\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0380 - accuracy: 0.9850 - fmeasure: 0.7707 - val_loss: 0.0510 - val_accuracy: 0.9810 - val_fmeasure: 0.7085\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0376 - accuracy: 0.9852 - fmeasure: 0.7710 - val_loss: 0.0509 - val_accuracy: 0.9809 - val_fmeasure: 0.7068\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0373 - accuracy: 0.9854 - fmeasure: 0.7764 - val_loss: 0.0509 - val_accuracy: 0.9809 - val_fmeasure: 0.7047\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0371 - accuracy: 0.9854 - fmeasure: 0.7773 - val_loss: 0.0512 - val_accuracy: 0.9810 - val_fmeasure: 0.7087\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 6s 50us/step - loss: 0.0369 - accuracy: 0.9855 - fmeasure: 0.7778 - val_loss: 0.0514 - val_accuracy: 0.9809 - val_fmeasure: 0.7039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 6/16 [10:56<18:33, 111.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 6s 43us/step - loss: 0.0614 - accuracy: 0.9773 - fmeasure: 0.6268 - val_loss: 0.0554 - val_accuracy: 0.9790 - val_fmeasure: 0.6627\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0550 - accuracy: 0.9793 - fmeasure: 0.6613 - val_loss: 0.0540 - val_accuracy: 0.9798 - val_fmeasure: 0.6789\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0528 - accuracy: 0.9801 - fmeasure: 0.6765 - val_loss: 0.0537 - val_accuracy: 0.9793 - val_fmeasure: 0.6428\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0512 - accuracy: 0.9805 - fmeasure: 0.6877 - val_loss: 0.0512 - val_accuracy: 0.9804 - val_fmeasure: 0.6788\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 6s 43us/step - loss: 0.0500 - accuracy: 0.9808 - fmeasure: 0.6929 - val_loss: 0.0510 - val_accuracy: 0.9805 - val_fmeasure: 0.6946\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0491 - accuracy: 0.9812 - fmeasure: 0.7013 - val_loss: 0.0519 - val_accuracy: 0.9801 - val_fmeasure: 0.6784\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0482 - accuracy: 0.9816 - fmeasure: 0.7098 - val_loss: 0.0507 - val_accuracy: 0.9808 - val_fmeasure: 0.6879\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0478 - accuracy: 0.9815 - fmeasure: 0.7071 - val_loss: 0.0503 - val_accuracy: 0.9802 - val_fmeasure: 0.6910\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0468 - accuracy: 0.9820 - fmeasure: 0.7141 - val_loss: 0.0506 - val_accuracy: 0.9803 - val_fmeasure: 0.6756\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0465 - accuracy: 0.9820 - fmeasure: 0.7168 - val_loss: 0.0511 - val_accuracy: 0.9804 - val_fmeasure: 0.6817\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0435 - accuracy: 0.9831 - fmeasure: 0.7373 - val_loss: 0.0500 - val_accuracy: 0.9809 - val_fmeasure: 0.7109\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0424 - accuracy: 0.9835 - fmeasure: 0.7426 - val_loss: 0.0509 - val_accuracy: 0.9806 - val_fmeasure: 0.6932\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0420 - accuracy: 0.9836 - fmeasure: 0.7449 - val_loss: 0.0506 - val_accuracy: 0.9810 - val_fmeasure: 0.7050\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0401 - accuracy: 0.9843 - fmeasure: 0.7552 - val_loss: 0.0505 - val_accuracy: 0.9809 - val_fmeasure: 0.7080\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0394 - accuracy: 0.9847 - fmeasure: 0.7649 - val_loss: 0.0506 - val_accuracy: 0.9810 - val_fmeasure: 0.7101\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0380 - accuracy: 0.9851 - fmeasure: 0.7708 - val_loss: 0.0506 - val_accuracy: 0.9809 - val_fmeasure: 0.7116\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0375 - accuracy: 0.9853 - fmeasure: 0.7747 - val_loss: 0.0514 - val_accuracy: 0.9809 - val_fmeasure: 0.6977\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0371 - accuracy: 0.9854 - fmeasure: 0.7779 - val_loss: 0.0513 - val_accuracy: 0.9810 - val_fmeasure: 0.7041\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0368 - accuracy: 0.9856 - fmeasure: 0.7784 - val_loss: 0.0514 - val_accuracy: 0.9810 - val_fmeasure: 0.7038\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 5s 42us/step - loss: 0.0365 - accuracy: 0.9856 - fmeasure: 0.7788 - val_loss: 0.0512 - val_accuracy: 0.9810 - val_fmeasure: 0.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████▍     | 7/16 [12:45<16:36, 110.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 6s 47us/step - loss: 0.0619 - accuracy: 0.9770 - fmeasure: 0.6224 - val_loss: 0.0554 - val_accuracy: 0.9789 - val_fmeasure: 0.6420\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0550 - accuracy: 0.9794 - fmeasure: 0.6628 - val_loss: 0.0539 - val_accuracy: 0.9796 - val_fmeasure: 0.6877\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0528 - accuracy: 0.9800 - fmeasure: 0.6748 - val_loss: 0.0531 - val_accuracy: 0.9799 - val_fmeasure: 0.6962\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0519 - accuracy: 0.9803 - fmeasure: 0.6810 - val_loss: 0.0518 - val_accuracy: 0.9803 - val_fmeasure: 0.6873\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0501 - accuracy: 0.9810 - fmeasure: 0.6947 - val_loss: 0.0515 - val_accuracy: 0.9802 - val_fmeasure: 0.6746\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0495 - accuracy: 0.9811 - fmeasure: 0.6982 - val_loss: 0.0502 - val_accuracy: 0.9807 - val_fmeasure: 0.6971\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0490 - accuracy: 0.9813 - fmeasure: 0.7011 - val_loss: 0.0517 - val_accuracy: 0.9803 - val_fmeasure: 0.6698\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0479 - accuracy: 0.9815 - fmeasure: 0.7061 - val_loss: 0.0518 - val_accuracy: 0.9804 - val_fmeasure: 0.6867\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0451 - accuracy: 0.9825 - fmeasure: 0.7239 - val_loss: 0.0500 - val_accuracy: 0.9808 - val_fmeasure: 0.6977\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0442 - accuracy: 0.9829 - fmeasure: 0.7319 - val_loss: 0.0509 - val_accuracy: 0.9805 - val_fmeasure: 0.6769\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0437 - accuracy: 0.9830 - fmeasure: 0.7350 - val_loss: 0.0492 - val_accuracy: 0.9809 - val_fmeasure: 0.7116\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0429 - accuracy: 0.9833 - fmeasure: 0.7403 - val_loss: 0.0512 - val_accuracy: 0.9804 - val_fmeasure: 0.6859\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0424 - accuracy: 0.9835 - fmeasure: 0.7425 - val_loss: 0.0502 - val_accuracy: 0.9808 - val_fmeasure: 0.6986\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0406 - accuracy: 0.9840 - fmeasure: 0.7531 - val_loss: 0.0510 - val_accuracy: 0.9808 - val_fmeasure: 0.6959\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0399 - accuracy: 0.9844 - fmeasure: 0.7580 - val_loss: 0.0502 - val_accuracy: 0.9808 - val_fmeasure: 0.7061\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0387 - accuracy: 0.9849 - fmeasure: 0.7685 - val_loss: 0.0502 - val_accuracy: 0.9810 - val_fmeasure: 0.7103\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0380 - accuracy: 0.9850 - fmeasure: 0.7712 - val_loss: 0.0510 - val_accuracy: 0.9808 - val_fmeasure: 0.7002\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0375 - accuracy: 0.9852 - fmeasure: 0.7733 - val_loss: 0.0514 - val_accuracy: 0.9808 - val_fmeasure: 0.7024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 8/16 [14:31<14:33, 109.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 6s 44us/step - loss: 0.0623 - accuracy: 0.9771 - fmeasure: 0.6258 - val_loss: 0.0556 - val_accuracy: 0.9785 - val_fmeasure: 0.6658\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0556 - accuracy: 0.9791 - fmeasure: 0.6588 - val_loss: 0.0549 - val_accuracy: 0.9796 - val_fmeasure: 0.6491\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0531 - accuracy: 0.9799 - fmeasure: 0.6743 - val_loss: 0.0520 - val_accuracy: 0.9802 - val_fmeasure: 0.6978\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0516 - accuracy: 0.9805 - fmeasure: 0.6823 - val_loss: 0.0512 - val_accuracy: 0.9804 - val_fmeasure: 0.6898\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0507 - accuracy: 0.9808 - fmeasure: 0.6878 - val_loss: 0.0517 - val_accuracy: 0.9805 - val_fmeasure: 0.7082\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0497 - accuracy: 0.9810 - fmeasure: 0.6949 - val_loss: 0.0509 - val_accuracy: 0.9805 - val_fmeasure: 0.6886\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0489 - accuracy: 0.9813 - fmeasure: 0.6997 - val_loss: 0.0508 - val_accuracy: 0.9803 - val_fmeasure: 0.6892\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 6s 44us/step - loss: 0.0480 - accuracy: 0.9816 - fmeasure: 0.7077 - val_loss: 0.0530 - val_accuracy: 0.9797 - val_fmeasure: 0.6537\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 6s 46us/step - loss: 0.0455 - accuracy: 0.9824 - fmeasure: 0.7211 - val_loss: 0.0514 - val_accuracy: 0.9804 - val_fmeasure: 0.6717\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 6s 45us/step - loss: 0.0445 - accuracy: 0.9829 - fmeasure: 0.7302 - val_loss: 0.0496 - val_accuracy: 0.9810 - val_fmeasure: 0.7074\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 6s 43us/step - loss: 0.0439 - accuracy: 0.9830 - fmeasure: 0.7341 - val_loss: 0.0502 - val_accuracy: 0.9805 - val_fmeasure: 0.6850\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0435 - accuracy: 0.9832 - fmeasure: 0.7386 - val_loss: 0.0504 - val_accuracy: 0.9806 - val_fmeasure: 0.6972\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0415 - accuracy: 0.9837 - fmeasure: 0.7453 - val_loss: 0.0501 - val_accuracy: 0.9811 - val_fmeasure: 0.7080\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0411 - accuracy: 0.9840 - fmeasure: 0.7507 - val_loss: 0.0503 - val_accuracy: 0.9808 - val_fmeasure: 0.7029\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0396 - accuracy: 0.9846 - fmeasure: 0.7618 - val_loss: 0.0500 - val_accuracy: 0.9810 - val_fmeasure: 0.7118\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0395 - accuracy: 0.9846 - fmeasure: 0.7635 - val_loss: 0.0502 - val_accuracy: 0.9809 - val_fmeasure: 0.7004\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0385 - accuracy: 0.9849 - fmeasure: 0.7675 - val_loss: 0.0505 - val_accuracy: 0.9809 - val_fmeasure: 0.7054\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0383 - accuracy: 0.9850 - fmeasure: 0.7694 - val_loss: 0.0506 - val_accuracy: 0.9809 - val_fmeasure: 0.7068\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0382 - accuracy: 0.9851 - fmeasure: 0.7718 - val_loss: 0.0508 - val_accuracy: 0.9810 - val_fmeasure: 0.7060\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 5s 43us/step - loss: 0.0377 - accuracy: 0.9852 - fmeasure: 0.7754 - val_loss: 0.0506 - val_accuracy: 0.9808 - val_fmeasure: 0.7034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████▋    | 9/16 [16:22<12:48, 109.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 4s 32us/step - loss: 0.0637 - accuracy: 0.9762 - fmeasure: 0.6256 - val_loss: 0.0548 - val_accuracy: 0.9789 - val_fmeasure: 0.6788\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0547 - accuracy: 0.9794 - fmeasure: 0.6696 - val_loss: 0.0565 - val_accuracy: 0.9792 - val_fmeasure: 0.6372\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0529 - accuracy: 0.9801 - fmeasure: 0.6828 - val_loss: 0.0534 - val_accuracy: 0.9797 - val_fmeasure: 0.7026\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0513 - accuracy: 0.9806 - fmeasure: 0.6928 - val_loss: 0.0530 - val_accuracy: 0.9798 - val_fmeasure: 0.6619\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0505 - accuracy: 0.9806 - fmeasure: 0.6936 - val_loss: 0.0524 - val_accuracy: 0.9800 - val_fmeasure: 0.6721\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0495 - accuracy: 0.9810 - fmeasure: 0.7045 - val_loss: 0.0503 - val_accuracy: 0.9805 - val_fmeasure: 0.7007\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0486 - accuracy: 0.9812 - fmeasure: 0.7059 - val_loss: 0.0503 - val_accuracy: 0.9806 - val_fmeasure: 0.6904\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0475 - accuracy: 0.9816 - fmeasure: 0.7140 - val_loss: 0.0499 - val_accuracy: 0.9807 - val_fmeasure: 0.7020\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0469 - accuracy: 0.9819 - fmeasure: 0.7174 - val_loss: 0.0500 - val_accuracy: 0.9807 - val_fmeasure: 0.7010\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0464 - accuracy: 0.9820 - fmeasure: 0.7203 - val_loss: 0.0522 - val_accuracy: 0.9804 - val_fmeasure: 0.6819\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0435 - accuracy: 0.9830 - fmeasure: 0.7397 - val_loss: 0.0498 - val_accuracy: 0.9809 - val_fmeasure: 0.7036\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0427 - accuracy: 0.9833 - fmeasure: 0.7459 - val_loss: 0.0502 - val_accuracy: 0.9807 - val_fmeasure: 0.7054\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0410 - accuracy: 0.9838 - fmeasure: 0.7532 - val_loss: 0.0504 - val_accuracy: 0.9808 - val_fmeasure: 0.7065\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0402 - accuracy: 0.9841 - fmeasure: 0.7595 - val_loss: 0.0504 - val_accuracy: 0.9808 - val_fmeasure: 0.7056\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0390 - accuracy: 0.9846 - fmeasure: 0.7673 - val_loss: 0.0501 - val_accuracy: 0.9807 - val_fmeasure: 0.7065\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0388 - accuracy: 0.9846 - fmeasure: 0.7689 - val_loss: 0.0505 - val_accuracy: 0.9809 - val_fmeasure: 0.7075\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0380 - accuracy: 0.9850 - fmeasure: 0.7746 - val_loss: 0.0510 - val_accuracy: 0.9808 - val_fmeasure: 0.7071\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0378 - accuracy: 0.9850 - fmeasure: 0.7763 - val_loss: 0.0509 - val_accuracy: 0.9808 - val_fmeasure: 0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████▎   | 10/16 [17:35<09:53, 98.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 7s 57us/step - loss: 0.0622 - accuracy: 0.9772 - fmeasure: 0.6256 - val_loss: 0.0549 - val_accuracy: 0.9792 - val_fmeasure: 0.6458\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 7s 57us/step - loss: 0.0552 - accuracy: 0.9794 - fmeasure: 0.6617 - val_loss: 0.0530 - val_accuracy: 0.9800 - val_fmeasure: 0.6711\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 8s 59us/step - loss: 0.0530 - accuracy: 0.9800 - fmeasure: 0.6779 - val_loss: 0.0527 - val_accuracy: 0.9801 - val_fmeasure: 0.6833\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0518 - accuracy: 0.9804 - fmeasure: 0.6809 - val_loss: 0.0519 - val_accuracy: 0.9801 - val_fmeasure: 0.6696\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 7s 57us/step - loss: 0.0504 - accuracy: 0.9808 - fmeasure: 0.6923 - val_loss: 0.0510 - val_accuracy: 0.9804 - val_fmeasure: 0.6873\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0492 - accuracy: 0.9811 - fmeasure: 0.7001 - val_loss: 0.0516 - val_accuracy: 0.9802 - val_fmeasure: 0.6673\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0483 - accuracy: 0.9815 - fmeasure: 0.7064 - val_loss: 0.0505 - val_accuracy: 0.9808 - val_fmeasure: 0.6991\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0477 - accuracy: 0.9817 - fmeasure: 0.7082 - val_loss: 0.0507 - val_accuracy: 0.9807 - val_fmeasure: 0.6959\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0468 - accuracy: 0.9821 - fmeasure: 0.7191 - val_loss: 0.0507 - val_accuracy: 0.9806 - val_fmeasure: 0.7009\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0440 - accuracy: 0.9830 - fmeasure: 0.7355 - val_loss: 0.0510 - val_accuracy: 0.9807 - val_fmeasure: 0.7050\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0429 - accuracy: 0.9833 - fmeasure: 0.7405 - val_loss: 0.0504 - val_accuracy: 0.9809 - val_fmeasure: 0.7035\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0423 - accuracy: 0.9836 - fmeasure: 0.7450 - val_loss: 0.0507 - val_accuracy: 0.9807 - val_fmeasure: 0.6984\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0416 - accuracy: 0.9838 - fmeasure: 0.7467 - val_loss: 0.0510 - val_accuracy: 0.9807 - val_fmeasure: 0.6998\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0395 - accuracy: 0.9844 - fmeasure: 0.7596 - val_loss: 0.0519 - val_accuracy: 0.9807 - val_fmeasure: 0.7012\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0388 - accuracy: 0.9848 - fmeasure: 0.7664 - val_loss: 0.0513 - val_accuracy: 0.9807 - val_fmeasure: 0.7109\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0375 - accuracy: 0.9854 - fmeasure: 0.7777 - val_loss: 0.0528 - val_accuracy: 0.9810 - val_fmeasure: 0.6995\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 7s 56us/step - loss: 0.0366 - accuracy: 0.9857 - fmeasure: 0.7825 - val_loss: 0.0521 - val_accuracy: 0.9809 - val_fmeasure: 0.7048\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0361 - accuracy: 0.9858 - fmeasure: 0.7839 - val_loss: 0.0523 - val_accuracy: 0.9808 - val_fmeasure: 0.7099\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0358 - accuracy: 0.9860 - fmeasure: 0.7871 - val_loss: 0.0525 - val_accuracy: 0.9808 - val_fmeasure: 0.7064\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 7s 55us/step - loss: 0.0354 - accuracy: 0.9860 - fmeasure: 0.7869 - val_loss: 0.0525 - val_accuracy: 0.9810 - val_fmeasure: 0.7089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|██████▉   | 11/16 [19:59<09:21, 112.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 4s 31us/step - loss: 0.0627 - accuracy: 0.9771 - fmeasure: 0.6339 - val_loss: 0.0618 - val_accuracy: 0.9765 - val_fmeasure: 0.5536\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0549 - accuracy: 0.9795 - fmeasure: 0.6707 - val_loss: 0.0548 - val_accuracy: 0.9796 - val_fmeasure: 0.6700\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0529 - accuracy: 0.9800 - fmeasure: 0.6813 - val_loss: 0.0534 - val_accuracy: 0.9797 - val_fmeasure: 0.6681\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0515 - accuracy: 0.9805 - fmeasure: 0.6924 - val_loss: 0.0562 - val_accuracy: 0.9790 - val_fmeasure: 0.6287\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0506 - accuracy: 0.9806 - fmeasure: 0.6944 - val_loss: 0.0505 - val_accuracy: 0.9806 - val_fmeasure: 0.6973\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0490 - accuracy: 0.9812 - fmeasure: 0.7060 - val_loss: 0.0509 - val_accuracy: 0.9805 - val_fmeasure: 0.6886\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0488 - accuracy: 0.9812 - fmeasure: 0.7065 - val_loss: 0.0507 - val_accuracy: 0.9806 - val_fmeasure: 0.6874\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0457 - accuracy: 0.9822 - fmeasure: 0.7270 - val_loss: 0.0496 - val_accuracy: 0.9809 - val_fmeasure: 0.7030\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0447 - accuracy: 0.9826 - fmeasure: 0.7337 - val_loss: 0.0517 - val_accuracy: 0.9806 - val_fmeasure: 0.6865\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0437 - accuracy: 0.9829 - fmeasure: 0.7393 - val_loss: 0.0503 - val_accuracy: 0.9809 - val_fmeasure: 0.6969\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0417 - accuracy: 0.9838 - fmeasure: 0.7537 - val_loss: 0.0497 - val_accuracy: 0.9810 - val_fmeasure: 0.7138\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0410 - accuracy: 0.9839 - fmeasure: 0.7555 - val_loss: 0.0501 - val_accuracy: 0.9809 - val_fmeasure: 0.7094\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0399 - accuracy: 0.9844 - fmeasure: 0.7657 - val_loss: 0.0501 - val_accuracy: 0.9812 - val_fmeasure: 0.7114\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0393 - accuracy: 0.9845 - fmeasure: 0.7674 - val_loss: 0.0499 - val_accuracy: 0.9809 - val_fmeasure: 0.7111\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0386 - accuracy: 0.9850 - fmeasure: 0.7749 - val_loss: 0.0506 - val_accuracy: 0.9811 - val_fmeasure: 0.7088\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0383 - accuracy: 0.9849 - fmeasure: 0.7745 - val_loss: 0.0502 - val_accuracy: 0.9810 - val_fmeasure: 0.7107\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0378 - accuracy: 0.9851 - fmeasure: 0.7768 - val_loss: 0.0506 - val_accuracy: 0.9810 - val_fmeasure: 0.7068\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 4s 30us/step - loss: 0.0378 - accuracy: 0.9851 - fmeasure: 0.7782 - val_loss: 0.0504 - val_accuracy: 0.9810 - val_fmeasure: 0.7095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▌  | 12/16 [21:09<06:38, 99.62s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0626 - accuracy: 0.9770 - fmeasure: 0.6318 - val_loss: 0.0554 - val_accuracy: 0.9783 - val_fmeasure: 0.6308\n",
      "Epoch 2/18\n",
      "127656/127656 [==============================] - 5s 35us/step - loss: 0.0543 - accuracy: 0.9794 - fmeasure: 0.6713 - val_loss: 0.0531 - val_accuracy: 0.9796 - val_fmeasure: 0.6876\n",
      "Epoch 3/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0524 - accuracy: 0.9801 - fmeasure: 0.6847 - val_loss: 0.0521 - val_accuracy: 0.9803 - val_fmeasure: 0.6789\n",
      "Epoch 4/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0514 - accuracy: 0.9804 - fmeasure: 0.6936 - val_loss: 0.0538 - val_accuracy: 0.9797 - val_fmeasure: 0.7016\n",
      "Epoch 5/18\n",
      "127656/127656 [==============================] - 5s 37us/step - loss: 0.0498 - accuracy: 0.9808 - fmeasure: 0.6986 - val_loss: 0.0511 - val_accuracy: 0.9803 - val_fmeasure: 0.6771\n",
      "Epoch 6/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0486 - accuracy: 0.9813 - fmeasure: 0.7080 - val_loss: 0.0528 - val_accuracy: 0.9806 - val_fmeasure: 0.6885\n",
      "Epoch 7/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0477 - accuracy: 0.9816 - fmeasure: 0.7152 - val_loss: 0.0504 - val_accuracy: 0.9804 - val_fmeasure: 0.7055\n",
      "Epoch 8/18\n",
      "127656/127656 [==============================] - 5s 37us/step - loss: 0.0468 - accuracy: 0.9819 - fmeasure: 0.7218 - val_loss: 0.0498 - val_accuracy: 0.9809 - val_fmeasure: 0.6987\n",
      "Epoch 9/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0460 - accuracy: 0.9822 - fmeasure: 0.7261 - val_loss: 0.0503 - val_accuracy: 0.9807 - val_fmeasure: 0.6988\n",
      "Epoch 10/18\n",
      "127656/127656 [==============================] - 5s 37us/step - loss: 0.0453 - accuracy: 0.9825 - fmeasure: 0.7327 - val_loss: 0.0507 - val_accuracy: 0.9809 - val_fmeasure: 0.7038\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0425 - accuracy: 0.9833 - fmeasure: 0.7463 - val_loss: 0.0500 - val_accuracy: 0.9808 - val_fmeasure: 0.7161\n",
      "Epoch 12/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0412 - accuracy: 0.9838 - fmeasure: 0.7557 - val_loss: 0.0498 - val_accuracy: 0.9812 - val_fmeasure: 0.7155\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 13/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0391 - accuracy: 0.9846 - fmeasure: 0.7692 - val_loss: 0.0508 - val_accuracy: 0.9807 - val_fmeasure: 0.7045\n",
      "Epoch 14/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0383 - accuracy: 0.9849 - fmeasure: 0.7747 - val_loss: 0.0504 - val_accuracy: 0.9809 - val_fmeasure: 0.7133\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0371 - accuracy: 0.9853 - fmeasure: 0.7806 - val_loss: 0.0505 - val_accuracy: 0.9811 - val_fmeasure: 0.7225\n",
      "Epoch 16/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0366 - accuracy: 0.9855 - fmeasure: 0.7858 - val_loss: 0.0514 - val_accuracy: 0.9809 - val_fmeasure: 0.7135\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 17/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0359 - accuracy: 0.9857 - fmeasure: 0.7885 - val_loss: 0.0518 - val_accuracy: 0.9808 - val_fmeasure: 0.7084\n",
      "Epoch 18/18\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0354 - accuracy: 0.9859 - fmeasure: 0.7916 - val_loss: 0.0520 - val_accuracy: 0.9809 - val_fmeasure: 0.7131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|████████▏ | 13/16 [22:33<04:44, 94.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 5s 37us/step - loss: 0.0625 - accuracy: 0.9772 - fmeasure: 0.6282 - val_loss: 0.0563 - val_accuracy: 0.9784 - val_fmeasure: 0.6231\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0544 - accuracy: 0.9795 - fmeasure: 0.6718 - val_loss: 0.0545 - val_accuracy: 0.9793 - val_fmeasure: 0.6484\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0527 - accuracy: 0.9802 - fmeasure: 0.6873 - val_loss: 0.0560 - val_accuracy: 0.9788 - val_fmeasure: 0.6319\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0511 - accuracy: 0.9807 - fmeasure: 0.6950 - val_loss: 0.0525 - val_accuracy: 0.9800 - val_fmeasure: 0.6687\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0501 - accuracy: 0.9809 - fmeasure: 0.7025 - val_loss: 0.0513 - val_accuracy: 0.9805 - val_fmeasure: 0.6999\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0489 - accuracy: 0.9814 - fmeasure: 0.7095 - val_loss: 0.0517 - val_accuracy: 0.9802 - val_fmeasure: 0.6694\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0478 - accuracy: 0.9816 - fmeasure: 0.7146 - val_loss: 0.0509 - val_accuracy: 0.9805 - val_fmeasure: 0.6829\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0470 - accuracy: 0.9817 - fmeasure: 0.7177 - val_loss: 0.0516 - val_accuracy: 0.9805 - val_fmeasure: 0.6791\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0463 - accuracy: 0.9820 - fmeasure: 0.7220 - val_loss: 0.0527 - val_accuracy: 0.9802 - val_fmeasure: 0.7090\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0432 - accuracy: 0.9831 - fmeasure: 0.7419 - val_loss: 0.0493 - val_accuracy: 0.9811 - val_fmeasure: 0.7113\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0421 - accuracy: 0.9836 - fmeasure: 0.7501 - val_loss: 0.0510 - val_accuracy: 0.9809 - val_fmeasure: 0.6956\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0414 - accuracy: 0.9838 - fmeasure: 0.7555 - val_loss: 0.0511 - val_accuracy: 0.9807 - val_fmeasure: 0.6894\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0395 - accuracy: 0.9844 - fmeasure: 0.7654 - val_loss: 0.0510 - val_accuracy: 0.9809 - val_fmeasure: 0.7070\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0386 - accuracy: 0.9848 - fmeasure: 0.7718 - val_loss: 0.0516 - val_accuracy: 0.9809 - val_fmeasure: 0.7050\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0376 - accuracy: 0.9851 - fmeasure: 0.7770 - val_loss: 0.0513 - val_accuracy: 0.9810 - val_fmeasure: 0.7099\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0371 - accuracy: 0.9854 - fmeasure: 0.7816 - val_loss: 0.0513 - val_accuracy: 0.9810 - val_fmeasure: 0.7146\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0362 - accuracy: 0.9855 - fmeasure: 0.7851 - val_loss: 0.0515 - val_accuracy: 0.9811 - val_fmeasure: 0.7122\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0358 - accuracy: 0.9859 - fmeasure: 0.7901 - val_loss: 0.0518 - val_accuracy: 0.9809 - val_fmeasure: 0.7091\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0355 - accuracy: 0.9860 - fmeasure: 0.7920 - val_loss: 0.0516 - val_accuracy: 0.9810 - val_fmeasure: 0.7162\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 5s 36us/step - loss: 0.0355 - accuracy: 0.9860 - fmeasure: 0.7916 - val_loss: 0.0515 - val_accuracy: 0.9810 - val_fmeasure: 0.7163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████▊ | 14/16 [24:06<03:08, 94.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 4s 35us/step - loss: 0.0632 - accuracy: 0.9766 - fmeasure: 0.6259 - val_loss: 0.0556 - val_accuracy: 0.9791 - val_fmeasure: 0.6940\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0542 - accuracy: 0.9796 - fmeasure: 0.6739 - val_loss: 0.0526 - val_accuracy: 0.9799 - val_fmeasure: 0.6641\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0523 - accuracy: 0.9801 - fmeasure: 0.6860 - val_loss: 0.0515 - val_accuracy: 0.9804 - val_fmeasure: 0.6902\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0510 - accuracy: 0.9805 - fmeasure: 0.6925 - val_loss: 0.0505 - val_accuracy: 0.9804 - val_fmeasure: 0.6927\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0496 - accuracy: 0.9808 - fmeasure: 0.6998 - val_loss: 0.0506 - val_accuracy: 0.9807 - val_fmeasure: 0.7010\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0484 - accuracy: 0.9814 - fmeasure: 0.7107 - val_loss: 0.0512 - val_accuracy: 0.9802 - val_fmeasure: 0.6876\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0458 - accuracy: 0.9823 - fmeasure: 0.7274 - val_loss: 0.0497 - val_accuracy: 0.9807 - val_fmeasure: 0.7055\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0446 - accuracy: 0.9827 - fmeasure: 0.7346 - val_loss: 0.0500 - val_accuracy: 0.9807 - val_fmeasure: 0.7079\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0438 - accuracy: 0.9830 - fmeasure: 0.7394 - val_loss: 0.0499 - val_accuracy: 0.9807 - val_fmeasure: 0.7048\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0417 - accuracy: 0.9837 - fmeasure: 0.7516 - val_loss: 0.0499 - val_accuracy: 0.9810 - val_fmeasure: 0.7128\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0413 - accuracy: 0.9839 - fmeasure: 0.7532 - val_loss: 0.0498 - val_accuracy: 0.9811 - val_fmeasure: 0.7096\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0396 - accuracy: 0.9845 - fmeasure: 0.7640 - val_loss: 0.0498 - val_accuracy: 0.9810 - val_fmeasure: 0.7212\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0390 - accuracy: 0.9847 - fmeasure: 0.7703 - val_loss: 0.0504 - val_accuracy: 0.9809 - val_fmeasure: 0.7096\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0383 - accuracy: 0.9849 - fmeasure: 0.7723 - val_loss: 0.0504 - val_accuracy: 0.9811 - val_fmeasure: 0.7174\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0380 - accuracy: 0.9849 - fmeasure: 0.7734 - val_loss: 0.0504 - val_accuracy: 0.9809 - val_fmeasure: 0.7150\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0374 - accuracy: 0.9853 - fmeasure: 0.7787 - val_loss: 0.0505 - val_accuracy: 0.9811 - val_fmeasure: 0.7152\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0374 - accuracy: 0.9852 - fmeasure: 0.7778 - val_loss: 0.0508 - val_accuracy: 0.9810 - val_fmeasure: 0.7144\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0371 - accuracy: 0.9856 - fmeasure: 0.7840 - val_loss: 0.0508 - val_accuracy: 0.9810 - val_fmeasure: 0.7136\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0370 - accuracy: 0.9854 - fmeasure: 0.7817 - val_loss: 0.0508 - val_accuracy: 0.9810 - val_fmeasure: 0.7147\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 4s 34us/step - loss: 0.0367 - accuracy: 0.9856 - fmeasure: 0.7850 - val_loss: 0.0508 - val_accuracy: 0.9810 - val_fmeasure: 0.7154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████▍| 15/16 [25:33<01:32, 92.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 4s 35us/step - loss: 0.0626 - accuracy: 0.9765 - fmeasure: 0.6266 - val_loss: 0.0589 - val_accuracy: 0.9782 - val_fmeasure: 0.6942\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0541 - accuracy: 0.9796 - fmeasure: 0.6741 - val_loss: 0.0520 - val_accuracy: 0.9802 - val_fmeasure: 0.6878\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0522 - accuracy: 0.9801 - fmeasure: 0.6860 - val_loss: 0.0518 - val_accuracy: 0.9800 - val_fmeasure: 0.6766\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0506 - accuracy: 0.9807 - fmeasure: 0.6969 - val_loss: 0.0511 - val_accuracy: 0.9803 - val_fmeasure: 0.6979\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0496 - accuracy: 0.9811 - fmeasure: 0.7041 - val_loss: 0.0509 - val_accuracy: 0.9805 - val_fmeasure: 0.6921\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0486 - accuracy: 0.9813 - fmeasure: 0.7104 - val_loss: 0.0502 - val_accuracy: 0.9808 - val_fmeasure: 0.7004\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0476 - accuracy: 0.9817 - fmeasure: 0.7171 - val_loss: 0.0510 - val_accuracy: 0.9806 - val_fmeasure: 0.6947\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0466 - accuracy: 0.9819 - fmeasure: 0.7205 - val_loss: 0.0502 - val_accuracy: 0.9803 - val_fmeasure: 0.6844\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0440 - accuracy: 0.9828 - fmeasure: 0.7366 - val_loss: 0.0503 - val_accuracy: 0.9808 - val_fmeasure: 0.6934\n",
      "Epoch 10/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0427 - accuracy: 0.9835 - fmeasure: 0.7474 - val_loss: 0.0512 - val_accuracy: 0.9804 - val_fmeasure: 0.7022\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 11/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0407 - accuracy: 0.9840 - fmeasure: 0.7588 - val_loss: 0.0499 - val_accuracy: 0.9809 - val_fmeasure: 0.7083\n",
      "Epoch 12/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0400 - accuracy: 0.9844 - fmeasure: 0.7639 - val_loss: 0.0499 - val_accuracy: 0.9808 - val_fmeasure: 0.7193\n",
      "Epoch 13/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0394 - accuracy: 0.9845 - fmeasure: 0.7663 - val_loss: 0.0503 - val_accuracy: 0.9809 - val_fmeasure: 0.7099\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0380 - accuracy: 0.9850 - fmeasure: 0.7749 - val_loss: 0.0506 - val_accuracy: 0.9809 - val_fmeasure: 0.7113\n",
      "Epoch 15/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0374 - accuracy: 0.9852 - fmeasure: 0.7782 - val_loss: 0.0510 - val_accuracy: 0.9809 - val_fmeasure: 0.7118\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 16/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0366 - accuracy: 0.9857 - fmeasure: 0.7857 - val_loss: 0.0513 - val_accuracy: 0.9809 - val_fmeasure: 0.7094\n",
      "Epoch 17/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0365 - accuracy: 0.9857 - fmeasure: 0.7855 - val_loss: 0.0517 - val_accuracy: 0.9810 - val_fmeasure: 0.7100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 18/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0359 - accuracy: 0.9859 - fmeasure: 0.7893 - val_loss: 0.0517 - val_accuracy: 0.9809 - val_fmeasure: 0.7092\n",
      "Epoch 19/20\n",
      "127656/127656 [==============================] - 4s 33us/step - loss: 0.0358 - accuracy: 0.9859 - fmeasure: 0.7891 - val_loss: 0.0514 - val_accuracy: 0.9809 - val_fmeasure: 0.7169\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Epoch 20/20\n",
      "127656/127656 [==============================] - 4s 35us/step - loss: 0.0354 - accuracy: 0.9861 - fmeasure: 0.7928 - val_loss: 0.0517 - val_accuracy: 0.9809 - val_fmeasure: 0.7118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 16/16 [26:59<00:00, 101.21s/it][A\n"
     ]
    }
   ],
   "source": [
    "# Define AGAIN the scan, this time implement a probability reduction\n",
    "scan_object = talos.Scan(x=x_train,\n",
    "                         y=y_train,\n",
    "                         x_val=x_val,\n",
    "                         y_val=y_val,\n",
    "                         params=params,\n",
    "                         reduction_method='forrest',\n",
    "                         reduction_metric='val_fmeasure',\n",
    "                         performance_target=['val_fmeasure', 0.8, False],\n",
    "                         model=toxic_clf,\n",
    "                         fraction_limit= 0.5,\n",
    "                         experiment_name='toxic_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testiamo ancora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy package toxic_comments_deploy_last have been saved.\n"
     ]
    }
   ],
   "source": [
    "talos.Deploy(scan_object=scan_object, model_name='toxic_comments_deploy_last', metric='val_fmeasure');\n",
    "\n",
    "model = talos.Restore('toxic_comments_deploy_last.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.99      0.74      0.85      6090\n",
      " severe_toxic       0.41      0.27      0.32       367\n",
      "      obscene       0.84      0.67      0.74      3691\n",
      "       threat       0.76      0.36      0.49       211\n",
      "       insult       0.79      0.62      0.69      3427\n",
      "identity_hate       0.75      0.32      0.45       712\n",
      "\n",
      "  avg / total       0.87      0.66      0.75     14498\n",
      "\n",
      "Micro avg:  (0.8769570823356051, 0.6567802455511105, 0.7510648367250355, None)\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.model.predict(test_vectors))\n",
    "c = classification_report(test_labels, y_pred, target_names=list_classes)\n",
    "print(c)\n",
    "\n",
    "score = precision_recall_fscore_support(test_labels, y_pred, average='micro')\n",
    "print('Micro avg: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprio quello che volevamo! abbiamo trovato una combinazione che ci ha permesso di superare il 75% di fmeasure in micro avg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlliamo ora gli iperparametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: \n",
      "Number of epochs:  15\n",
      "Batch size:  256\n",
      "Third hidden:  50\n",
      "Second hidden:  200\n",
      "First hidden:  500\n"
     ]
    }
   ],
   "source": [
    "bp = analyze_object.best_params('val_fmeasure', exclude=['val_loss', 'val_accuracy', 'loss', \n",
    "                                                         'accuracy','fmeasure', 'lr'])\n",
    "print(\"Best params: \")\n",
    "print(\"Number of epochs: \",bp[0][1])\n",
    "print(\"Batch size: \", bp[0][4])\n",
    "print(\"Third hidden: \", bp[0][2])\n",
    "print(\"Second hidden: \", bp[0][5])\n",
    "print(\"First hidden: \", bp[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Osserviamo infine l'andamento del best seen per quanto riguarda Fmeasure e Val Fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_seen(best_seen, iteration, measure):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel(measure)\n",
    "    plt.step(iteration, best_seen)\n",
    "    plt.ylim(best_seen[0]-0.002, best_seen[len(best_seen)-1]+0.002)\n",
    "    plt.xticks(np.arange(0, len(iteration)+1, 1)) \n",
    "    plt.show()\n",
    "\n",
    "def create_bestseen(values):\n",
    "    best_seen = []\n",
    "    iteration = []\n",
    "    i = 0\n",
    "\n",
    "    for val in values:\n",
    "        if (i == 0): best_seen.append(val)\n",
    "        elif (val > best_seen[i-1]): best_seen.append(val)\n",
    "        else: best_seen.append(best_seen[i-1])\n",
    "        i += 1\n",
    "        iteration.append(i)\n",
    "\n",
    "    return best_seen, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEMCAYAAACP9hKLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dfVhUZf4/8PfMADMDgyYjTDComcNDaTxYCFSaWRomiJSpmNW39edVrWZZlq2yrdt3v5u1W/SAWJlagCZt62pgWtqTtt8YFAFLERnUdBAcxZQBZGaYmd8fXczXCXmYB/C0vF/X1ZXn5r7f5x4umA/nnHvOEdntdjuIiIgEQny1J0BERHQ5FiYiIhIUFiYiIhIUFiYiIhIUFiYiIhIUn6s9gf8EIpHoak+BiOg36UoLw1mYvMSbq+6NRiMCAwOZxzzmMe8/Oq+rP+p5Ko+IiASFhYmIiASFhYmIiASFhYmIiASl3xY/tLS0oKCgAFVVVVAoFEhPT0dCQkKnfjk5OaitrXVst7e3Q6VSISsrCwBQVFSEyspKNDQ0ICUlBampqY6+O3fuxOeff+7YttlsaG9vxyuvvAKFQoG8vDzs27cPPj7/97Jfe+01iMWsz0REQtFvhamwsBASiQSrVq2CXq9Hbm4u1Go1wsLCnPotWrTIaTs7OxtRUVGO7eDgYGRkZGDv3r2d9pGSkoKUlBTHdnFxMXQ6HRQKhaNt8uTJmD59urdeFhEReVm/HCqYTCaUl5cjLS0NMpkMGo0GMTExKC0t7XZcY2MjdDodEhMTHW1JSUkYPXo0pFJpt2Ptdju0Wi2SkpK88hqIiKh/9MsRk8FggFgshkqlcrSp1WrU1NR0O06r1UKj0UCpVLq8T51Oh+bmZsTFxTm179mzB3v27IFSqURKSgri4+Ndzr4So9HolRwAaG5u9loW85jHPOb9lvKAfipMJpMJcrncqU0ul8NkMnU7TqvVOp2ac0VJSQni4+Mhk8kcbRMnTsR9990HuVyOqqoqrFu3DoMGDcKoUaPc2sflvPmBNeYxj3nMG8h5/XIqTyqV4tKlS05tbW1t3Z6O0+l0aGpqcuuIxmw2o7y83OkUIAAMHz4cCoUCEokEY8aMQUJCAioqKlzOJyKivtMvhSkkJAQ2mw0Gg8HRptfrOy18uJxWq0VsbKzTEU9vVVRUwN/fH5GRkd32E4lEXr2VEBERea7fjpji4uJQXFwMk8mE2tpaHDx4EOPGjbtif7PZjLKyMiQnJ3f6mtVqhcVigd1uh81mg8Vigc1mc+pTUlKCxMTETvdhOnDgANra2mCz2XD48GGUlpYiJibGey+UiIg81m/LxefMmYP8/HwsW7YMAQEByMzMRFhYGHQ6HVavXo3s7GxH38rKyi6PeDZu3IiSkhLH9s6dO/HQQw85itiFCxdw9OhRzJkzp9PYr7/+GgUFBQAApVKJBx98sMejKiIi6l8iO89leczbpwSFdPdf5jGPeczrq7yu3jt5ywMiIhIUFiYiIhIUFiYiIhIUFiYiIhIUFiYiIhKUflsuTkQ0EDyx+QdYbKKeO/aS1WqFRCIRbN4QuQTvPHzlz6S6i4WJiMiL2iw2fPL7272WJ6Tl3V3leRtP5RERkaCwMBERkaCwMBERkaCwMBERkaCwMBERkaCwMBERkaCwMBERkaCwMBERkaCwMBERkaD0250fWlpaUFBQgKqqKigUCqSnpyMhIaFTv5ycHNTW1jq229vboVKpkJWVBQAoKipCZWUlGhoakJKSgtTUVEffo0eP4s0334Sfn5+jbfbs2UhKSnJpDkREdPX0W2EqLCyERCLBqlWroNfrkZubC7VajbCwMKd+ixYtctrOzs5GVFSUYzs4OBgZGRnYu3fvFfczePBg/PWvf/VoDkREdPX0y6k8k8mE8vJypKWlQSaTQaPRICYmBqWlpd2Oa2xshE6nQ2JioqMtKSkJo0ePhlQq7Zc5EBFR/+qXIyaDwQCxWAyVSuVoU6vVqKmp6XacVquFRqOBUqns9b6MRiOWLVsGPz8/xMbGIi0tDVKp1O05uLJfb2lubvZaFvOYx7z+zbNarXw/8FC/FCaTyQS5XO7UJpfLYTKZuh2n1WqRkpLS6/2oVCosX74cKpUK58+fR15eHv75z39i7ty5bs+ht7x5t17mMY95v908iUQi6Pn9FvL65VSeVCrFpUuXnNra2tq6PR2n0+nQ1NSE+Pj4Xu9n8ODBCA0NhVgsxtChQ5GRkYHy8nK350BERP2vXwpTSEgIbDYbDAaDo02v13e76ECr1SI2NhYymcyjfdvtdrfnQERE/a/fjpji4uJQXFwMk8mE2tpaHDx4EOPGXfmph2azGWVlZUhOTu70NavVCovFArvdDpvNBovFApvNBgCorq5GY2Mj7HY7zp8/j61btyImJsatORAR0dXRb8vF58yZg/z8fCxbtgwBAQHIzMxEWFgYdDodVq9ejezsbEffyspK+Pv7IzIyslPOxo0bUVJS4tjeuXMnHnroISQnJ0Ov1+ODDz5Aa2srAgICEBcXh+nTp/c4ByIiEg6RveNcF7lNJBLBm9/G38KjlJnHPOZd2czc7wbco9XdzevqvZO3JCIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkHpt8deEBEJ0RObf4DFJvJansyXf+97ioWJiAa0NovN64+pIM+wtBMRkaD02xFTS0sLCgoKUFVVBYVCgfT0dCQkJHTql5OTg9raWsd2e3s7VCoVsrKyAABFRUWorKxEQ0MDUlJSkJqa6uj7ww8/4IsvvsDp06fh6+uLMWPGYObMmZDJZACAvLw87Nu3Dz4+//eyX3vtNYjFrM9ERELRb4WpsLAQEokEq1atgl6vR25uLtRqdadHmy9atMhpOzs7G1FRUY7t4OBgZGRkYO/evZ320dbWhpSUFERERKC9vR3r16/Hli1bMHfuXEefyZMnOz1unYiIhKVfDhVMJhPKy8uRlpYGmUwGjUaDmJgYlJaWdjuusbEROp0OiYmJjrakpCSMHj0aUqm0U/+EhASMHj0afn5+8Pf3x2233YZjx455/fUQEVHf6ZcjJoPBALFYDJVK5WhTq9WoqanpdpxWq4VGo4FSqXRrvzqdDqGhoU5te/bswZ49e6BUKpGSkoL4+Hi3sn/Nmxc8m5ubvZbFPOYxr3tWq5W/vwLKA/qpMJlMJsjlcqc2uVwOk8nU7TitVouUlBS39llVVYWSkhI8//zzjraJEyfivvvug1wuR1VVFdatW4dBgwZh1KhRbu3jcoGBgR5nMI95zOv/PIlEIuj5DcS8filMUqkUly5dcmpra2u74um4DjqdDk1NTW4d0Rw/fhwbNmzAggULnI7Shg8f7vj3mDFjkJCQgIqKCq8UJiHz9uc0rFYrJBIJ85j3H5HHzx0JT78UppCQENhsNhgMBoSEhAAA9Hp9p4UPl9NqtYiNjXWsqOutU6dOYc2aNZg3bx6io6O77SsSiWC3213K/y3qi89pePMvJOYx72rnkbD0y58KUqkUcXFxKC4uhslkQm1tLQ4ePIhx48Zdsb/ZbEZZWRmSk5M7fc1qtcJiscBut8Nms8FiscBmswEATp8+jZycHMyaNQsxMTGdxh44cABtbW2w2Ww4fPgwSktLr9iPiIiunn5bLj5nzhzk5+dj2bJlCAgIQGZmJsLCwqDT6bB69WpkZ2c7+lZWVsLf3x+RkZGdcjZu3IiSkhLH9s6dO/HQQw8hOTkZu3fvRnNzMzZu3IiNGzcCAIKCgvDHP/4RAPD111+joKAAAKBUKvHggw9ecR9ERHT1iOwD4VxWH/P2KUFvn6qYmfsdT+Uxj3nME1xeV++dvOpHRESCwsJERESCwsJERESCwsJERESC4vKqvCNHjuAf//gHGhoasHr1ahw5cgRms5nLromIyCtcOmL6xz/+gQkTJqCurg75+fkAfrlP0jPPPNMnkyMiooHHpcL04osvYteuXXjnnXcctwSJjY1FZWVln0yOiIgGHpcKk8FgcJyyE4lEjv93/JuIiMhTLhWmm2++2XEKr8PmzZu7vLUQERGRq1xa/PDWW29hypQpWLduHVpaWnDPPffg6NGj+OKLL/pqfkRENMC4VJiio6Nx5MgRFBcXIzU1FcOGDUNqaioUCkVfzY+IiAaYXhcmq9WKyMhIHD58GLNmzerLORER0QDW62tMEokEEomk0wP/iIiIvMmlU3lPP/00Zs+ejeXLlyM8PNxpNd7111/v9ckREdHA41JhWrRoEQBg165dTu0ikQhWq9V7syIiogHLpcLU8aRYIiKivtJvT7BtaWlBQUEBqqqqoFAokJ6ejoSEhE79cnJyUFtb69hub2+HSqVCVlYWAKCoqAiVlZVoaGhASkoKUlNTncbv27cP27ZtQ3NzM6Kjo/HQQw8hICDApTkQEdHV41JhGj9+fJd3edizZ0+3YwsLCyGRSLBq1Sro9Xrk5uZCrVYjLCzMqV/H6cIO2dnZiIqKcmwHBwcjIyMDe/fu7bSP06dPY9OmTfj973+PYcOGYdOmTdi8eTPmz5/v0hyIiOjqcenOD//v//0/zJ8/3/HftGnT0NDQgLvvvrvbcSaTCeXl5UhLS4NMJoNGo0FMTAxKS0u7HdfY2AidTofExERHW1JSEkaPHg2pVNqp/759+3DTTTchIiICMpkMaWlpqKioQFtbm9tzICKi/uXSEdMjjzzSqe3+++/Ho48+ihdffLHLcQaDAWKxGCqVytGmVqtRU1PT7f60Wi00Gg2USmWv5ldfX++0OjA4OBg+Pj4wGAwQiURuzaG3jEajV3KAX+7Y7k1Wq1XQ82Me85jHvMt5fI1JrVbj4MGD3fYxmUyQy+VObXK5HCaTqdtxWq0WKSkpvZ6LyWSCTCZzapPJZGhra4NYLHZrDr0VGBjolZy+yJNIJIKeH/OYxzzmXc6lwrR+/Xqn7dbWVmzZsgVJSUndjpNKpZ0+mNvW1nbF03EddDodmpqaEB8f3+v5SaVStLW1ddqPTCaDSCRyeQ5ERNT/XCpMv76zeEBAAG699VYsWbKk23EhISGw2WwwGAwICQkBAOj1+m4XHWi1WsTGxnY6AupOaGgo6urqHNvnzp1De3s7QkJCIBKJXJ4DERH1P5cK09dff+3WTqRSKeLi4lBcXIwHH3wQer0eBw8exNKlS6/Y32w2o6ysDI899linr1mtVthsNtjtdthsNlgsFkgkEojFYiQkJODvf/87dDodhg0bhqKiIsTFxTmKmytzICKiq8OlwnT48GEolUqoVCo0Nzfjb3/7G8RiMZ577jn4+/t3O3bOnDnIz8/HsmXLEBAQgMzMTISFhUGn02H16tXIzs529K2srIS/vz8iIyM75WzcuBElJSWO7Z07d+Khhx5CcnIywsLCkJmZiQ0bNqClpcXxOaae5kBERMIhstvt9t52jo2Nxccff4yoqCg8/vjjqK6uhkwmw9ChQzud5htIRCIRXPg29shoNHr1YuLM3O/wye9v91qet+fHPOYxb2DmdfXe6dIR04kTJxAVFQW73Y4tW7bg8OHDkMvlGDlypFuTIiIi+jWXCpNMJoPRaMThw4cxfPhwDB06FO3t7Z1WwhEREbnLpcI0d+5cTJo0CUaj0XHroAMHDvCIiYiIvMalwpSdnY0vvvgCvr6+uPPOOwEAYrHYaeECERGRJ1y+88OUKVOctm+55RavTYaIiMilwtTe3o7c3Fx8++23OHfunNNqip7uLk5ERNQbLt1dfMmSJXj33XcxYcIElJWV4f7774fBYMCkSZP6an5ERDTAuFSYtmzZgh07duCpp56Cj48PnnrqKWzdutXtO0IQERH9mkuFqbW1FcOGDQPwy525W1tbER0djfLy8j6ZHBERDTwuXWO64YYbsG/fPowbNw633HILVq5ciUGDBkGtVvfV/IiIaIBxqTC9+eabkEgkAIDXX38dTzzxBIxGI957770+mRwREQ08LhWmhIQEx78jIiKwe/dur0+IiIgGNpeuMQHArl27MH/+fKSlpQEA9u/fj6+++srrEyMiooHJpcL09ttv44knnkBERITjc0tyuRxZWVl9MjkiIhp4XCpMb7zxBnbv3o0XXngBYvEvQ6Ojo1FdXd0nkyMiooHHpcJkNBody8VFIhEAwGKxwM/Pz/szIyKiAcmlxQ8TJkzAqlWrsGLFCkfbW2+95biha3daWlpQUFCAqqoqKBQKpKenOy2m6JCTk4Pa2lrHdnt7O1QqleN0YWNjI/Ly8nDixAkEBQVh9uzZiI6OBgBs2rQJ+/btc4y1Wq2QSCSOm8xmZ2fj+PHjjpWFgwcPxsqVK135FhARUR9zqTC9/fbbSEtLw9q1a2E0GhEVFYXAwEAUFxf3OLawsBASiQSrVq2CXq9Hbm4u1Gp1p0ebdzxOo0N2djaioqIc2+vXr8fIkSOxcOFCHDp0CGvXrsXKlSsRGBiIuXPnYu7cuY6+eXl5jiO7DrNnz8Ztt93myssmIqJ+5NKpvNDQUOzbtw+FhYXYtGkTPvzwQ5SWluLaa6/tdpzJZEJ5eTnS0tIgk8mg0WgQExOD0tLSbsc1NjZCp9MhMTERAHDmzBmcOnUKqamp8PPzQ3x8PMLCwq5454mOfXaMJSKi3waXH3shEomQmJjo0hu+wWCAWCyGSqVytKnVatTU1HQ7TqvVQqPRQKlUAgDq6+uhVCohk8kcfcLDw1FfX99pbHl5ORQKBSIiIpzat23bhq1bt0KlUmH69OmIjIzs9esgIqK+51JhqqysxJIlS1BRUYHm5mYAgN1uh0gkgtls7nKcyWSCXC53apPL5TCZTN3uT6vVIiUlpdscmUyGixcvXnFsYmKi06m8GTNmIDQ0FBKJBGVlZVizZg2WL1+O4ODgbufRG0aj0eOMDh3fW2+xWq2Cnh/zmMc85l3OpcKUmZmJ+++/H2+99VanAtEdqVSKS5cuObW1tbVBKpV2OUan06GpqQnx8fFOOW1tbT3mnD9/HkePHsWDDz7o1H75I+CTkpKwf/9+/Pjjj71avNGTwMBAjzP6Kk8ikQh6fsxjHvOYdzmXClNDQwNeeumlTgsKehISEgKbzQaDwYCQkBAAgF6v77Tw4XJarRaxsbFOp+1CQ0Nx7tw5tLW1Odrr6uo6PUVXq9Vi1KhRGDp0qEvzJCKiq8+lxQ+PPPIINm3a5PJOpFIp4uLiUFxcDJPJhNraWhw8eBDjxo27Yn+z2YyysjIkJyc7tatUKoSHh2P79u2wWCyoqKhAXV2d01EV8EthSkpKcmprbW3F4cOHYbFYYLVaUVpaCp1OhxtvvNHl10NERH3HpSOmF154AcnJyfjrX//qtJABQI/3y5szZw7y8/OxbNkyBAQEIDMzE2FhYdDpdFi9erXjs0bAL9ey/P39r7gwYf78+cjLy8PSpUsxZMgQLFiwwOkw8tixY7hw4QLGjh3rNM5qteLTTz/FmTNnHAsxHnvssU6vg4iIri6R3W6397bz+PHj4efnh4yMjE7XmObPn+/1yf1WiEQiuPBt7JHRaPTqOduZud/hk9/f7rU8b8+Pecxj3sDM6+q906UjpoqKCjQ2NvIWRERE1GdcusY0fvx4HD58uK/mQkRE5NoR08iRIzFlyhRkZGR0ujbz0ksveXViREQ0MLlUmFpbWzFt2jSYzWacOnXK0e7q8nEiIqKu9FiYcnJyHDdWXbFiBTQaTZ9PioiIBq4eC9OKFSschWns2LFoamrq80kNdE9s/gEWm/eOQmW+Ll1KJCK6qnosTNdffz2effZZjB49GhaLBevXr79iv9/97nden9xA1WaxeX15NxHRb0WPhamwsBCvvvoqPvroI1gsFuTn53fqIxKJWJiIiMgreixMkZGReP/99wEAd911F7788stu++v1eoSHh3tndkRENOC4dPGhp6IEgPeeIyIij3j9qrg3b81DREQDj9cLEz/TREREnuA6YiIiEhQWJiIiEhReYyIiIkHxemHi3ceJiMgTPX6OadiwYb1a0HDy5ElH/ytpaWlBQUEBqqqqoFAokJ6ejoSEhE79cnJyUFtb69hub2+HSqVCVlYWAKCxsRF5eXk4ceIEgoKCMHv2bERHRwMAvv/+exQUFDg9L+qJJ55wPAm3u7FERCQMPRamgoICr+yosLAQEokEq1atgl6vR25uLtRqNcLCwpz6ddyXr0N2djaioqIc2+vXr8fIkSOxcOFCHDp0CGvXrsXKlSsdT1DsuIXSlfQ0loiIrr4eC9Mdd9zh8U5MJhPKy8uRlZUFmUwGjUaDmJgYlJaWYsaMGV2Oa2xshE6nw8MPPwwAOHPmDE6dOoUnn3wSfn5+iI+Px1dffYXy8nJMmDCh2zl4MpaIiPqPS89jAn55vPrevXtx7tw5p4UO3T0o0GAwQCwWOz1cUK1Wo6amptt9abVaaDQaKJVKAEB9fT2USiVkMpmjT3h4OOrr6x3bp06dwnPPPYeAgACMGzcO99xzDyQSSa/GEhHR1edSYXrvvfewZMkSTJkyBTt27MDUqVPxxRdfID09vdtxJpMJcrncqU0ul8NkMnU7TqvVIiUlpdscmUyGixcvAgAiIiKQlZWFoKAg1NfXY926dRCLxUhJSelxrKe8eQdvq9Xq1bzm5mavZTGPecxjXl/mAS4WpldffRU7d+7E+PHjMWTIEPzrX//Cjh07sHnz5m7HSaVSXLp0yamtra0NUqm0yzE6nQ5NTU2Ij493ymlra+syZ+jQoY52tVqNe++9F7t27UJKSkqPYz3lzetUEonE69e9mMc85jHvt5Ln0nJxg8GA8ePH/zJQLIbNZsPUqVNRVFTU7biQkBDYbDYYDAZHm16v77Tw4XJarRaxsbFOp95CQ0Nx7tw5pwJTV1eH0NDQHufuyVgiIuo/LhWm8PBwHD9+HMAvj8PYtm0b9u7d67Q8+0qkUini4uJQXFwMk8mE2tpaHDx4EOPGjbtif7PZjLKyMiQnJzu1q1QqhIeHY/v27bBYLKioqEBdXZ3jqOrQoUOOJ+w2NDRgx44diImJ6dVYIiISBpdO5T3//PM4cuQIRo4ciRdffBEzZ86E2WzGW2+91ePYOXPmID8/H8uWLUNAQAAyMzMRFhYGnU6H1atXIzs729G3srIS/v7+js8fXW7+/PnIy8vD0qVLMWTIECxYsMBxGHnkyBHk5eXBZDIhMDAQ48aNc7pG1d1YIiISBpHdhXsIPf3005g7d67jSMdsNsNsNkOhUPTZBH8LRCKRV2/FNDP3O68/Wt2bBZh5zGMe87yR19V7p8u3JJoxYwYiIiLwpz/9CSdOnBjwRYmIiLzLpcL0xhtvOO7acOrUKSQmJuLmm2/G66+/3lfzIyKiAcblIyaxWIzJkydj/fr1+PHHH6FUKvHcc8/1xdyIiGgAcrkwddyMddq0aYiMjISPjw8+/PDDvpgbERENQC6tynvggQewY8cOjB07FpmZmfjwww+dPtRKRETkKZcKU0JCAl577TUMHz68r+ZDREQDnMufYyIiIupLXn+CLRERkSdYmIiISFBYmIiISFBYmIiISFBYmIiISFBYmIiISFBYmIiISFBYmIiISFBYmIiISFBYmIiISFBcuiWRJzruSl5VVQWFQoH09HQkJCR06peTk4Pa2lrHdnt7O1QqFbKysgAAjY2NyMvLw4kTJxAUFITZs2cjOjoaAFBSUoKvv/4aZ8+ehUwmwy233IL09HRIJBIAQHZ2No4fP+7YHjx4MFauXNnHr5yIiFzRb4WpsLAQEokEq1atcjxsUK1WIywszKnfokWLnLazs7MRFRXl2F6/fj1GjhyJhQsX4tChQ1i7di1WrlyJwMBAmM1mPPDAA7juuuvQ3NyMNWvWYPfu3bjnnnsc42fPno3bbrutb18sERG5rV9O5ZlMJpSXlyMtLQ0ymQwajQYxMTEoLS3tdlxjYyN0Oh0SExMBAGfOnMGpU6eQmpoKPz8/xMfHIywsDOXl5QCACRMmQKPRwMfHB9dccw3GjRvndPRFRETC1y9HTAaDAWKxGCqVytGmVqtRU1PT7TitVguNRgOlUgkAqK+vh1KphEwmc/QJDw9HfX39FcfX1NR0OiLbtm0btm7dCpVKhenTpyMyMtLdl+XEaDR6JQcArFarV/Oam5u9lsU85jGPeX2ZB/RTYTKZTJDL5U5tcrkcJpOp23FarRYpKSnd5shkMly8eLHT2P/93//FyZMnMW/ePEfbjBkzEBoaColEgrKyMqxZswbLly9HcHCwOy/LSWBgoMcZHSQSiVfzAO/Oj3nMYx7z+jKvX07lSaVSXLp0yamtra0NUqm0yzE6nQ5NTU2Ij493ymlra+sxp6KiAtu2bcPChQuhUCgc7SNHjoRMJoOvry+SkpIwatQo/Pjjj568NCIi8rJ+KUwhISGw2WwwGAyONr1e3+k02+W0Wi1iY2OdTtuFhobi3LlzTsWprq4OoaGhju1Dhw5h06ZNeOKJJ6BWq738SoiIqK/12xFTXFwciouLYTKZUFtbi4MHD2LcuHFX7G82m1FWVobk5GSndpVKhfDwcGzfvh0WiwUVFRWoq6tzHFVVV1fjgw8+wIIFC3Ddddc5jW1tbcXhw4dhsVhgtVpRWloKnU6HG2+8sU9eMxERuafflovPmTMH+fn5WLZsGQICApCZmYmwsDDodDqsXr0a2dnZjr6VlZXw9/e/4sKE+fPnIy8vD0uXLsWQIUOwYMECx/nNzz77DJcuXUJubq6j/6hRo7Bo0SJYrVZ8+umnOHPmjGMhxmOPPea0IIOIiK4+kd1ut1/tSfzWiUQiePPbODP3O3zy+9u9lmc0Gr16cZJ5zGMe87yR19V7J29JREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgsLCREREgtJvT7BtaWlBQUEBqqqqoFAokJ6ejoSEhE79cnJyUFtb69hub2+HSqVCVlYWAKCxsRF5eXk4ceIEgm6BPSwAABKGSURBVIKCMHv2bERHRzv6f/nll9i1axfMZjPi4+MxZ84c+Pr69mosERFdff1WmAoLCyGRSLBq1Sro9Xrk5uZCrVYjLCzMqd+iRYuctrOzsxEVFeXYXr9+PUaOHImFCxfi0KFDWLt2LVauXInAwEAcPnwYX3zxBZ566ilcc801ePfdd7F9+3bMmDGjx7FERCQM/XIqz2Qyoby8HGlpaZDJZNBoNIiJiUFpaWm34xobG6HT6ZCYmAgAOHPmDE6dOoXU1FT4+fkhPj4eYWFhKC8vBwCUlJTg1ltvRVhYGPz9/TF16lSUlJT0aiwREQlDvxwxGQwGiMViqFQqR5tarUZNTU2347RaLTQaDZRKJQCgvr4eSqUSMpnM0Sc8PBz19fWOr8fExDh9rampCc3NzT2O9ZTRaPRKDgBYrVav5jU3N3sti3nMYx7z+jIP6KfCZDKZIJfLndrkcjlMJlO347RaLVJSUrrNkclkuHjx4hW/3vFvk8nU41hPefN0oEQi8frpReYxj3nM+63k9cupPKlUikuXLjm1tbW1QSqVdjlGp9OhqakJ8fHxTjltbW1d5vz66x37lEqlPY4lIiJh6JfCFBISApvNBoPB4GjT6/WdFj5cTqvVIjY21unUW2hoKM6dO+dUYOrq6hAaGur4ul6vd/raoEGDoFAoehxLRETC0G9HTHFxcSguLobJZEJtbS0OHjyIcePGXbG/2WxGWVkZkpOTndpVKhXCw8Oxfft2WCwWVFRUoK6uznFUlZiYiO+//x719fVobW3Fjh07kJSU1KuxREQkDP32Ads5c+bAbDZj2bJlWL9+PTIzMxEWFgadToclS5Y49a2srIS/vz8iIyM75cyfPx8nT57E0qVLsXXrVixYsMBxfnP06NGYPHky3njjDWRlZSEoKAjTpk3r1VghGarwu9pTICK6akR2u91+tSfxWycSieDNb6PRaPRqwWQe85jHPCHmdfXeyVsSERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoLAwERGRoPhc7Qn8pxCJRFd7CkRE/xFYmLzAbrdf7SkQEf3H4Kk8IiISFBYmIiISFBYmIiISFBYmIiISFBYmIiISFBYmIiISFBYmIiISFH6OyQMtLS0oKChAVVUVFAoF0tPTkZCQ4FbWN998g5KSEpw+fRq33HILHn74YY/mZrFYsHnzZlRXV6OlpQXBwcFIT0/H6NGj3c7csGEDqqurYTabMWjQIEyePBm33XabR/MEAIPBgL/85S+Ij4/Ho48+6nZOdnY2jh8/DolEAgAYPHgwVq5c6dHc9u/fj+3bt+Pnn3/GoEGD8PDDD0Oj0bics2TJEqdts9mMCRMmYPbs2W7PrbGxEZs3b8axY8fg6+uL+Ph4zJw50/H6XVVfX4/CwkKcPHkSgYGByMjIQFxcXK/Gdvfze+TIERQWFuL8+fO47rrr8PDDD0OpVLqV197ejg0bNuCnn37C+fPn8fTTTyMyMtLt+R0/fhxFRUU4efIkxGIxIiIiMGvWLAwePNitvPr6enz44Yc4e/YsAGD48OGYNWsWQkND3cq73GeffYbi4mIsXrwY0dHRbuU1Njbij3/8I6RSqaPv5MmTce+997o9P7PZjH/+8584cOAArFYrwsPD8cwzz3Sb1xMWJg8UFhZCIpFg1apV0Ov1yM3NhVqtRlhYmMtZgwcPRkpKCqqqqmCxWDyem81mw5AhQ7BkyRIMGTIEhw4dwvvvv4+srKwe3xS6cs8992DevHnw9fVFQ0MD3njjDQwbNgzDhw/3aK6bN2/GiBEjPMroMHv2bK8USwCoqqrC1q1bMX/+fIwYMQJNTU1uZ2VnZzv+3dbWhj/84Q8YO3asR/PbvHkzAgMDsWrVKrS2tuLtt9/Gnj17cOedd7qcZbVa8e6772L8+PFYvHgxampqsGbNGvzhD3+ASqXqcXxXP7/Nzc147733MG/ePNx0000oKirCunXr8Pzzz7uVBwCjRo3CnXfeiffff7/Xr6+rvNbWVtx+++244YYbIJFIUFhYiPz8fCxatMitvMGDB2PBggUICgqC3W7Ht99+i3Xr1iErK8vt1wsAZ8+exYEDB3osmL3N+/vf/+7SHzDd5W3cuBE2mw0vvvgiAgICoNfre53bFZ7Kc5PJZEJ5eTnS0tIgk8mg0WgQExOD0tJSt/Li4+MRFxeHgIAAr8xPKpUiNTUVSqUSYrEYN910E5RKJU6ePOl2ZlhYGHx9fQH83y2YOv4ydNf+/fvh7++PqKgoj3L6QnFxMaZOnYqRI0dCLBbjmmuuwTXXXONxbkVFBRQKhVtHXpc7d+4cxo4dC19fXwwePBg33ngj6uvr3co6c+YMLl68iEmTJkEsFiMqKgrXX399r3+eu/r5raioQGhoqGOe06ZNQ11dHRoaGtzK8/HxwaRJk6DRaCAW9/7tq6u80aNHY+zYsZDL5fDz88Mdd9yB2tpat/P8/f2hVCohEolgt9shFot79TvS0+9/YWEhZsyY0eti4u33k67yGhoa8MMPP2Du3LkIDAyEWCz2+A9VgEdMbjMYDBCLxU5/TarVatTU1FzFWXWtqakJBoOhx1MKPfnoo49QUlICi8WCYcOGeXRq8NKlSyguLsZTTz2Ff//73x7Nq8O2bduwdetWqFQqTJ8+vVenea7EZrPh5MmTiImJwZ/+9CdYLBbExsYiIyMDfn5+Hs2xpKQEiYmJHt9fcdKkSdi/fz8iIyPR2tqKQ4cOIS0tzaPMXzt9+rTH48PDwx3bUqkUQ4cORX19Pa699lpPp+d1Op3O498RAHj22WdhMplgt9uRmprqUdaBAwfg4+ODMWPGeDyvDllZWRCJRIiOjsZ9990HhULhVs6JEycQFBSE7du3Q6vVYvDgwZg2bRri4+M9mh8Lk5tMJhPkcrlTm1wuh8lkukoz6prVasWGDRuQlJTk8ZtBZmYmZs+ejWPHjqGmpsZxBOWOoqIi3HrrrRgyZIhHc+owY8YMhIaGQiKRoKysDGvWrMHy5csRHBzsclZTUxOsVivKy8vxzDPPQCKR4J133sGOHTuQnp7u9hwbGxtRU1ODefPmuZ3RQaPR4LvvvsMzzzwDm82GpKQkxMbGupWlUqmgUCiwa9cu3HXXXaiurkZNTY3bhb2DyWRCYGCgU5tcLkdbW5tHuX1Br9fjs88+w+OPP+5x1muvvQaTyYSSkhIEBQW5ndPW1oZt27Zh8eLFHs8JAAICArBs2TKEh4ejpaUFhYWF2LBhA5588km38i5cuIDTp08jLi4OL7/8Mo4dO4Y1a9bg2muv9ajA81Sem6RSKS5duuTU1tbW5nRRUQhsNhs++OAD+Pj4eHSh/XJisRgajQY///wz9uzZ41bGqVOnUF1djUmTJnllTgAwcuRIyGQy+Pr6IikpCaNGjcKPP/7oVlbHUdHEiRMxePBgKBQK3HXXXTh06JBHcywtLcWoUaMwdOhQj3JsNhtWr16NuLg4ZGdn49VXX0Vrayv+9a9/uZUnkUjw2GOP4ccff8QLL7yAL7/8EmPHjvX41GVXvycymcyjXG8zGAxYvXo1HnjgAY9PsXaQSqUYP3488vLyYDQa3crYvn07EhMT3b4u/GsymQwjRoyARCLBoEGDMGvWLFRVVbn9h4Kvry8kEgmmTp0KHx8fREZGIiIiAlVVVR7Nk0dMbgoJCYHNZoPBYEBISAiAX/7icmfhQ1+x2+0oKChAU1MTFi5c6PZqra7YbDa3rzHV1NSgsbHRcVHYZDLBZrPh5Zdfxh/+8AdvTtMt/v7+Xrme9GtarRZTpkzxOKe1tRXnz5/HxIkT4evr6yjGRUVFuO+++9zK/PVqqr/97W9ISkryaJ5hYWEoKSlxbJtMJpw9e9Yrp8u8pbGxEW+99RamTp2KxMREr2bb7XaYzWZcuHCh05Fjb1RXVzv9AWg0GvH+++9jypQpXvk56jid7O4TEtRqdZeZnmBhcpNUKkVcXByKi4vx4IMPQq/X4+DBg1i6dKlbeVarFTabzfGfxWKBWCz2qJh89NFHaGhowOLFiz2+LmI0GlFdXY0xY8bAz88PR44cwf79+/G73/3Orbzbb78dN998s2N79+7dOH/+PObMmeNWXmtrK06cOIGIiAiIxWKUlZVBp9PhgQcecCsPAJKTk/HNN99g9OjRkEgk+Oqrr3DTTTe5nVdbW4sLFy54vBoPABQKBZRKJfbs2YO7774bJpMJWq32im8UvaXX66FSqRyryZqamnpdmLr6+Y2NjcWWLVtQXl6OMWPG4LPPPoNare7xlHJ3vw+Xrwprb2+HxWKBj49Pt2+IXeUZjUa8+eabuOOOOzBhwoTefaO6yTt69CgUCgXUajVMJhOKiorg7+/v9utdvHgxrFaro98rr7yCmTNn4sYbb3Qr7+TJk/D390dwcDBaW1vx8ccfIyIiotNlid7mRUREICgoCJ9//jnuuecenDhxAkePHkVGRkavv5dXIrLzYUJua2lpQX5+Po4cOYKAgADMmDHD7c8xFRcX47PPPnNqu/fee92+cNrxeQUfHx+n4paZmYlx48a5nGc0GrF27VrU1dXBbrcjKCgIEydOxO233+7W/H6tuLgYZ8+edftzTEajEatXr8aZM2cci1LS0tJwww03uD0nq9WKjz/+GPv374evry/Gjh2LjIwMt6+rbdq0CWazGf/1X//l9pwud+rUKXzyySfQ6/WOlXSzZs3CoEGD3MrbsmUL/v3vf8Nms2HUqFGYNWuW42xAT7r7+XXnc0zd5WVlZeH8+fNOX/vv//7vbjO7yhOJRNi+fXunU/CXL+93JS8sLAxFRUW4cOECfH19MWLECKSnpzstAHEl79e//1lZWZg3b16Pn2PqKk+lUuHTTz+F0WiETCZDdHQ0MjIyelyG3t38Tp8+jY0bN6Kurg5BQUGYPn16rz//1hUWJiIiEhQufiAiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIiIkFhYSIawBQKBY4dO3a1p0HkhIWJ6Cq67rrrsHv3bnzwwQde+7ByVyZOnNjpGUbNzc24/vrr+3S/RK5iYSL6D9De3n61p0DkNSxMRFdZVVUVHn/8cXz//fdQKBSOm8eaTCYsXboUw4cPh0qlwuOPP+64U/c333yD8PBwvPLKK7j22mvx6KOP4ueff0ZqaiqCg4MxZMgQpKamOp4mumLFCuzduxeLFi2CQqFwPKFVJBJBp9MBAC5evIiHH34YwcHBGDFiBP7yl7/AZrMBgOOIbunSpRgyZAhGjhyJHTt29Pe3igYIFiaiq+yGG27AO++8g+TkZDQ3N+PChQsAgBdeeAFHjx5FRUUFdDod6urq8NJLLznGNTQ04Pz58/jpp5/w3nvvwWaz4dFHH8VPP/2EkydPQi6XOwrQ//zP/2D8+PHIyclBc3MzcnJyOs3jySefxMWLF3Hs2DF8++23yMvLw4YNGxxf12q1iIqKwrlz5/D8889j/vz5bt+Vmqg7LExEAmS32/Hee+8hOzsbQUFBCAwMxPLly7F582ZHH7FYjD//+c+QSqWQy+VQKpW4//774e/vj8DAQKxYsQLffvttr/ZntVqxefNmvPzyywgMDMR1112HZ599Fvn5+Y4+I0aMwIIFCyCRSPDII4+gvr4eZ86c8fprJ+JjL4gE6OzZs2htbXV6NIjdbnd6BEJwcLDTA/daW1uxZMkS7Ny5Ez///DOAX+66brVae3x8yrlz52CxWDBixAhH24gRI1BXV+fYvvzRDf7+/gB+WTxB5G08YiISgF8/S2jo0KGQy+U4dOgQLly4gAsXLuDixYtOheDXY1577TVUV1dDq9WiqanJ8XC5jtNt3T2vaOjQofD19cVPP/3kaDt58qRHz3cichcLE5EAqFQq6PV6mM1mAL+cpluwYAGWLFkCg8EAAKirq8Pnn3/eZYbRaIRcLsc111yD8+fP489//nOnfXT1mSWJRIJZs2ZhxYoVMBqN+Omnn/D6669j3rx5XnqFRL3HwkQkAJMmTcLo0aNx7bXXYujQoQB+eVqpRqNBUlISBg0ahLvvvhvV1dVdZjz99NO4dOkShg4diqSkJKSkpDh9/amnnsInn3yCIUOGYPHixZ3Gv/322wgICMD111+P22+/HXPnznX7CcVEnuCDAomISFB4xERERILCwkRERILCwkRERILCwkRERILCwkRERILCwkRERILCwkRERILCwkRERILy/wFIgMWa/T2Z3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH = path.abspath(\"\")+'/toxic_comments_deploy_last/'\n",
    "\n",
    "df = pd.read_csv(PATH+'toxic_comments_deploy_last_results.csv')\n",
    "\n",
    "val_fmeasure = df['val_fmeasure']\n",
    "fmeasure = df['fmeasure']\n",
    "\n",
    "best_seen_valf, iteration_valf = create_bestseen(val_fmeasure)\n",
    "best_seen_f, iteration_f = create_bestseen(fmeasure)\n",
    "\n",
    "plot_best_seen(best_seen_valf, iteration_valf, 'val_fmeasure')\n",
    "plot_best_seen(best_seen_f, iteration_f, 'fmeasure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Osserviamo alcuni errori: in posizione 101 ad esempio il commento non è stato classificato come insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real labels:  [1. 0. 0. 0. 1. 0.]\n",
      "Prediction:  [1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Real labels: \",test_labels.values[101])\n",
    "print(\"Prediction: \",y_pred[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlliamo il commento originale per capirne di più"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('dataset/test.csv')\n",
    "temp_test_labels = pd.read_csv('dataset/test_labels.csv')\n",
    "test_combined = pd.merge(test_data, temp_test_labels, on=['id'])\n",
    "\n",
    "# Now remove -1 and 0 values\n",
    "test_data = test_combined[(test_combined.toxic == 1) | (test_combined.severe_toxic == 1) |\n",
    "                      (test_combined.obscene == 1) | (test_combined.threat == 1) | (test_combined.insult == 1) |\n",
    "                      (test_combined.identity_hate == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I sometimes wonder why some people feel they are entitled to act any way they want, I was not bothering you, and yet you have started this.  I don't think it is any of your concern what I do...who are you to police me??  Like your so innocent, You are constantly stirring up the pot and causing trouble, so save me your so innocent act and quit being a hypocrite....wanna be revolutionary, you think your so perfect and righteous and everyone is wrong except you...your a biased fanatic and you make me sick, your a spoiled baby and I want you to quit interfering in what I do!!\n"
     ]
    }
   ],
   "source": [
    "comments = test_data.comment_text.values\n",
    "print(comments[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, questo commento è abbastanza ambiguo, ci può stare un errore su questo, anche perchè osservandolo non sembrerebbe nemmeno un insulto ma un commento in risposta a qualcuno che ha effettivamente insultato.\n",
    "\n",
    "### Osserviamone un altro, il 108 è stato interamente sbagliato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real labels:  [1. 0. 0. 0. 1. 1.]\n",
      "Prediction:  [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Real labels: \",test_labels.values[108])\n",
    "print(\"Prediction: \",y_pred[108])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look, you're a pedant, and Fetzer is a Jew-hater. On Press TV in the UK today - September 2nd 2011 - he said that the Israelis were behind 9/11. The man is a complete fool.\n"
     ]
    }
   ],
   "source": [
    "print(comments[108])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anche in questo caso probabilmente c'è un errore nella etichettatura. A tal proposito, alcune veloci considerazioni possono essere che, se è stata una macchina ad etichettare, allora probabilmente vi era un bias verso alcune parole (jew -> identity hate), altrimenti se è stato un umano allora ha proprio sbagliato. Ci sta etichettare questo commento come tossico, alla fine sta criticando/insultando qualcuno, ma etichettarlo come identity hate è eccessivo.\n",
    "\n",
    "### Osserviamo ora il 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real labels:  [1. 0. 1. 0. 0. 0.]\n",
      "Prediction:  [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Real labels: \",test_labels.values[104])\n",
    "print(\"Prediction: \",y_pred[104])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CHANGES TO DUARTE ARE TRUE == \n",
      "\n",
      " Wikipedia sucks.\n"
     ]
    }
   ],
   "source": [
    "print(comments[104])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In questo caso, l'etichetta oscena sembra eccessiva. \"wikipedia fa schifo\" ci sta come commento tossico, ma non osceno.\n",
    "\n",
    "### Osserviamo ora un caso diverso, ovvero una situazione in cui il nostro classificatore ha etichettato commenti che non andavano etichettati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real labels:  [1. 0. 0. 0. 1. 0.]\n",
      "Prediction:  [1. 1. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Real labels: \",test_labels.values[105])\n",
    "print(\"Prediction: \",y_pred[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup my peeps japan sucks hola from mexico senioritas you suuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuucccccccccccccccccccccccccccccccccckkkkkkkkkkkkkkk\n"
     ]
    }
   ],
   "source": [
    "print(comments[105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questo commento pare molto più osceno rispetto al commento precedente. Probabilmente tutte queste lettere in aggiunta hanno enfatizzato la tossicità nel nostro classificatore.\n",
    "\n",
    "### Osserviamo il numero 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real labels:  [1. 0. 1. 0. 1. 0.]\n",
      "Prediction:  [1. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Real labels: \",test_labels.values[2000])\n",
    "print(\"Prediction: \",y_pred[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":You're a so stupid guy.\n"
     ]
    }
   ],
   "source": [
    "print(comments[2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anche in questo caso, il tag obscene sembra eccessivo. E' un commento tossico relativo a un insulto, il nostro classificatore ha classificato bene.\n",
    "\n",
    "### Osserviamo un ultimo commento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real labels:  [1. 0. 1. 0. 1. 0.]\n",
      "Prediction:  [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Real labels: \",test_labels.values[2016])\n",
    "print(\"Prediction: \",y_pred[2016])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \n",
      " :Yeah, wouldn't that be fun if we'd just let you put your junk wherever it pleases you; as for the \"\"clown\"\"-label, read WP:NPA.   \"\n"
     ]
    }
   ],
   "source": [
    "print(comments[2016])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questo è un altro commento estremamente ambiguo, difficile da interpretare perfino per un umano.\n",
    "\n",
    "### Concludendo, è evidente che molte etichette siano state utilizzate male/a sproposito, quindi ci aspettiamo che il nostro classificatore funzioni in realtà meglio di quanto visto nel classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
